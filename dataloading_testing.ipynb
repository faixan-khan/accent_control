{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docopt import docopt\n",
    "\n",
    "import sys\n",
    "import gc\n",
    "import platform\n",
    "from os.path import dirname, join\n",
    "from tqdm import tqdm, trange\n",
    "from datetime import datetime\n",
    "\n",
    "# The deepvoice3 model\n",
    "from deepvoice3_pytorch import frontend, builder\n",
    "import audio\n",
    "import lrschedule\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils import data as data_utils\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import numpy as np\n",
    "from numba import jit\n",
    "\n",
    "from nnmnkwii.datasets import FileSourceDataset, FileDataSource\n",
    "from os.path import join, expanduser\n",
    "import random\n",
    "\n",
    "# import librosa.display\n",
    "import matplotlib\n",
    "# hopefully this solves the error\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "from matplotlib import cm\n",
    "from warnings import warn\n",
    "from hparams import hparams, hparams_debug_string\n",
    "import cv2\n",
    "\n",
    "global_step = 0\n",
    "global_epoch = 0\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    cudnn.benchmark = False\n",
    "\n",
    "_frontend = None  # to be set later\n",
    "\n",
    "\n",
    "def _pad(seq, max_len, constant_values=0):\n",
    "    return np.pad(seq, (0, max_len - len(seq)),\n",
    "                  mode='constant', constant_values=constant_values)\n",
    "\n",
    "\n",
    "def _pad_2d(x, max_len, b_pad=0):\n",
    "    x = np.pad(x, [(b_pad, max_len - len(x) - b_pad), (0, 0)],\n",
    "               mode=\"constant\", constant_values=0)\n",
    "    return x\n",
    "\n",
    "def _pad_4d(x, max_len):\n",
    "    idx = x.shape[0]\n",
    "    y = np.zeros((max_len,48,96,3))\n",
    "    y[:idx] = x\n",
    "    return y\n",
    "\n",
    "def plot_alignment(alignment, path, title=None, info=None):\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(\n",
    "        alignment,\n",
    "        aspect='auto',\n",
    "        origin='lower',\n",
    "        interpolation='none')\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    xlabel = 'Decoder timestep'\n",
    "    if info is not None:\n",
    "        xlabel += '\\n\\n' + info\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Encoder timestep')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, format='png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextDataSource(FileDataSource):    \n",
    "    def __init__(self, data_root, speaker_id=None):        \n",
    "        self.data_root = data_root        \n",
    "        self.speaker_ids = None\n",
    "        self.multi_speaker = False\n",
    "        # If not None, filter by speaker_id\n",
    "        self.speaker_id = speaker_id\n",
    "\n",
    "    def collect_files(self):\n",
    "        meta = join(self.data_root, \"train.txt\")\n",
    "        with open(meta, \"rb\") as f:\n",
    "            lines = f.readlines()\n",
    "        l = lines[0].decode(\"utf-8\").split(\"|\")\n",
    "        assert len(l) == 5 or len(l) == 6\n",
    "        self.multi_speaker = len(l) == 6\n",
    "        texts = list(map(lambda l: l.decode(\"utf-8\").split(\"|\")[3], lines))\n",
    "        if self.multi_speaker:\n",
    "            speaker_ids = list(map(lambda l: int(l.decode(\"utf-8\").split(\"|\")[-1]), lines))\n",
    "            # Filter by speaker_id\n",
    "            # using multi-speaker dataset as a single speaker dataset\n",
    "            if self.speaker_id is not None:\n",
    "                indices = np.array(speaker_ids) == self.speaker_id\n",
    "                texts = list(np.array(texts)[indices])\n",
    "                self.multi_speaker = False\n",
    "                return texts\n",
    "\n",
    "            return texts, speaker_ids\n",
    "        else:\n",
    "            return texts\n",
    "\n",
    "    def collect_features(self, *args):\n",
    "        if self.multi_speaker:\n",
    "            text, speaker_id = args\n",
    "        else:\n",
    "            text = args[0]\n",
    "        global _frontend\n",
    "        if _frontend is None:\n",
    "            _frontend = getattr(frontend, hparams.frontend)\n",
    "        seq = _frontend.text_to_sequence(text, p=hparams.replace_pronunciation_prob)\n",
    "\n",
    "        if platform.system() == \"Windows\":\n",
    "            if hasattr(hparams, 'gc_probability'):\n",
    "                _frontend = None  # memory leaking prevention in Windows\n",
    "                if np.random.rand() < hparams.gc_probability:\n",
    "                    gc.collect()  # garbage collection enforced\n",
    "                    print(\"GC done\")\n",
    "\n",
    "        if self.multi_speaker:\n",
    "            return np.asarray(seq, dtype=np.int32), int(speaker_id)\n",
    "        else:\n",
    "            return np.asarray(seq, dtype=np.int32)\n",
    "\n",
    "\n",
    "class _NPYDataSource(FileDataSource):\n",
    "    def __init__(self, data_root, col, speaker_id=None):\n",
    "        self.data_root = data_root\n",
    "        self.col = col\n",
    "        self.frame_lengths = []\n",
    "        self.speaker_id = speaker_id\n",
    "\n",
    "    def collect_files(self):\n",
    "        meta = join(self.data_root, \"train.txt\")\n",
    "        with open(meta, \"rb\") as f:\n",
    "            lines = f.readlines()\n",
    "        l = lines[0].decode(\"utf-8\").split(\"|\")\n",
    "        assert len(l) == 5 or len(l) == 6\n",
    "        multi_speaker = len(l) == 6\n",
    "        self.frame_lengths = list(\n",
    "            map(lambda l: int(l.decode(\"utf-8\").split(\"|\")[2]), lines))\n",
    " \n",
    "        paths = list(map(lambda l: l.decode(\"utf-8\").split(\"|\")[self.col], lines))\n",
    "        paths = list(map(lambda f: join(self.data_root, f), paths))\n",
    " \n",
    "        if multi_speaker and self.speaker_id is not None:\n",
    "            speaker_ids = list(map(lambda l: int(l.decode(\"utf-8\").split(\"|\")[-1]), lines))\n",
    "            # Filter by speaker_id\n",
    "            # using multi-speaker dataset as a single speaker dataset\n",
    "            indices = np.array(speaker_ids) == self.speaker_id\n",
    "            paths = list(np.array(paths)[indices])\n",
    "            self.frame_lengths = list(np.array(self.frame_lengths)[indices])\n",
    "            # aha, need to cast numpy.int64 to int\n",
    "            self.frame_lengths = list(map(int, self.frame_lengths))\n",
    " \n",
    "        return paths\n",
    " \n",
    "    def collect_features(self, path):\n",
    "        return np.load(path)\n",
    "\n",
    "class MelSpecDataSource(_NPYDataSource):\n",
    "    def __init__(self, data_root, speaker_id=None):\n",
    "        super(MelSpecDataSource, self).__init__(data_root, 1, speaker_id)\n",
    "\n",
    "\n",
    "class LinearSpecDataSource(_NPYDataSource):\n",
    "    def __init__(self, data_root, speaker_id=None):\n",
    "        super(LinearSpecDataSource, self).__init__(data_root, 0, speaker_id)\n",
    "\n",
    "class VisualDataSource(FileDataSource):\n",
    "    def __init__(self, data_root, speaker_id=None):\n",
    "        self.data_root = data_root \n",
    "        self.speaker_ids = None\n",
    "        self.multi_speaker = False\n",
    "        # If not None, filter by speaker_id\n",
    "        self.speaker_id = speaker_id\n",
    "\n",
    "    def collect_files(self):\n",
    "        meta = join(self.data_root, \"train.txt\")\n",
    "        with open(meta, \"rb\") as f:\n",
    "            lines = f.readlines()\n",
    "        l = lines[0].decode(\"utf-8\").split(\"|\")\n",
    "        assert len(l) == 5 or len(l) == 6\n",
    "        self.multi_speaker = len(l) == 6\n",
    "        video_file = list(map(lambda l: l.decode(\"utf-8\").split(\"|\")[4].strip(), lines))\n",
    "        if self.multi_speaker:\n",
    "            speaker_ids = list(map(lambda l: int(l.decode(\"utf-8\").split(\"|\")[-1]), lines))\n",
    "            # Filter by speaker_id\n",
    "            # using multi-speaker dataset as a single speaker dataset\n",
    "            if self.speaker_id is not None:\n",
    "                indices = np.array(speaker_ids) == self.speaker_id\n",
    "                texts = list(np.array(texts)[indices])\n",
    "                self.multi_speaker = False\n",
    "                return texts\n",
    "\n",
    "            return video_file, speaker_ids\n",
    "        else:\n",
    "            return video_file\n",
    "\n",
    "    def collect_features(self, path):\n",
    "#             add wavs to path and then read the video files\n",
    "#         path = path = './wavs/'\n",
    "        path = os.path.join(self.data_root, path)\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        # frameWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        # frameHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "        # video = np.empty((frameCount, frameHeight, frameWidth, 3), np.dtype('uint8'))\n",
    "\n",
    "        ret = True\n",
    "        video = []\n",
    "        fc = 0\n",
    "        while fc < frameCount and ret:\n",
    "            ret, frame = cap.read()\n",
    "            video.append(frame)\n",
    "            fc += 1\n",
    "        cap.release()\n",
    "        video.append(np.ones(frame.shape, dtype=frame.dtype)*255) # stop frame, all white\n",
    "        x = np.asarray(video) / 255.\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class PartialyRandomizedSimilarTimeLengthSampler(Sampler):\n",
    "    \"\"\"Partially randmoized sampler\n",
    "\n",
    "    1. Sort by lengths\n",
    "    2. Pick a small patch and randomize it\n",
    "    3. Permutate mini-batchs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lengths, batch_size=16, batch_group_size=None,\n",
    "                 permutate=True):\n",
    "        self.lengths, self.sorted_indices = torch.sort(torch.LongTensor(lengths))\n",
    "        self.batch_size = batch_size\n",
    "        if batch_group_size is None:\n",
    "            batch_group_size = min(batch_size * 32, len(self.lengths))\n",
    "            if batch_group_size % batch_size != 0:\n",
    "                batch_group_size -= batch_group_size % batch_size\n",
    "\n",
    "        self.batch_group_size = batch_group_size\n",
    "        assert batch_group_size % batch_size == 0\n",
    "        self.permutate = permutate\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = self.sorted_indices.clone()\n",
    "        batch_group_size = self.batch_group_size\n",
    "        s, e = 0, 0\n",
    "        for i in range(len(indices) // batch_group_size):\n",
    "            s = i * batch_group_size\n",
    "            e = s + batch_group_size\n",
    "            random.shuffle(indices[s:e])\n",
    "\n",
    "        # Permutate batches\n",
    "        if self.permutate:\n",
    "            perm = np.arange(len(indices[:e]) // self.batch_size)\n",
    "            random.shuffle(perm)\n",
    "            indices[:e] = indices[:e].view(-1, self.batch_size)[perm, :].view(-1)\n",
    "\n",
    "        # Handle last elements\n",
    "        s += batch_group_size\n",
    "        if s < len(indices):\n",
    "            random.shuffle(indices[s:])\n",
    "\n",
    "        return iter(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sorted_indices)\n",
    "\n",
    "\n",
    "class PyTorchDataset(object):\n",
    "    def __init__(self, X, Mel, Y, V):\n",
    "        self.X = X\n",
    "        self.Mel = Mel\n",
    "        self.Y = Y\n",
    "        self.V = V\n",
    "        # alias\n",
    "        self.multi_speaker = X.file_data_source.multi_speaker\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        if self.multi_speaker:\n",
    "            text, speaker_id = self.X[idx]\n",
    "            return text, self.Mel[idx], self.Y[idx], speaker_id\n",
    "        else:\n",
    "            return self.X[idx], self.Mel[idx], self.Y[idx], self.V[idx]\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "\n",
    "def sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.arange(0, max_len).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = sequence_length.unsqueeze(1) \\\n",
    "        .expand_as(seq_range_expand)\n",
    "    return (seq_range_expand < seq_length_expand).float()\n",
    "\n",
    "\n",
    "class MaskedL1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedL1Loss, self).__init__()\n",
    "        self.criterion = nn.L1Loss(reduction=\"sum\")\n",
    "\n",
    "    def forward(self, input, target, lengths=None, mask=None, max_len=None):\n",
    "        if lengths is None and mask is None:\n",
    "            raise RuntimeError(\"Should provide either lengths or mask\")\n",
    "\n",
    "        # (B, T, 1)\n",
    "        if mask is None:\n",
    "            mask = sequence_mask(lengths, max_len).unsqueeze(-1)\n",
    "\n",
    "        # (B, T, D)\n",
    "        mask_ = mask.expand_as(input)\n",
    "        loss = self.criterion(input * mask_, target * mask_)\n",
    "        return loss / mask_.sum()\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Create batch\"\"\"\n",
    "    r = hparams.outputs_per_step\n",
    "    downsample_step = hparams.downsample_step\n",
    "    multi_speaker = len(batch[0]) == 5\n",
    "\n",
    "    # Lengths\n",
    "    input_lengths = [len(x[0]) for x in batch]\n",
    "    max_input_len = max(input_lengths)\n",
    "\n",
    "    target_lengths = [len(x[1]) for x in batch]\n",
    "\n",
    "    max_target_len = max(target_lengths)\n",
    "    if max_target_len % r != 0:\n",
    "        max_target_len += r - max_target_len % r\n",
    "        assert max_target_len % r == 0\n",
    "    if max_target_len % downsample_step != 0:\n",
    "        max_target_len += downsample_step - max_target_len % downsample_step\n",
    "        assert max_target_len % downsample_step == 0\n",
    "  \n",
    "      # Set 0 for zero beginning padding\n",
    "      # imitates initial decoder states\n",
    "    b_pad = r\n",
    "    max_target_len += b_pad * downsample_step\n",
    "\n",
    "    a = np.array([_pad(x[0], max_input_len) for x in batch], dtype=np.int)\n",
    "    x_batch = torch.LongTensor(a)\n",
    "\n",
    "    input_lengths = torch.LongTensor(input_lengths)\n",
    "    target_lengths = torch.LongTensor(target_lengths)\n",
    "\n",
    "    b = np.array([_pad_2d(x[1], max_target_len, b_pad=b_pad) for x in batch],\n",
    "              dtype=np.float32)\n",
    "    mel_batch = torch.FloatTensor(b)\n",
    "\n",
    "    c = np.array([_pad_2d(x[2], max_target_len, b_pad=b_pad) for x in batch],\n",
    "              dtype=np.float32)\n",
    "    y_batch = torch.FloatTensor(c)\n",
    "    \n",
    "    ## VIDEO ADDITION ##\n",
    "    vid_input_lengths = [x[3].shape[0] for x in batch]\n",
    "    vid_input_lengths = torch.LongTensor(vid_input_lengths)\n",
    "    max_vid_len = max(vid_input_lengths)\n",
    "    d = np.array([_pad_4d(x[3], max_vid_len) for x in batch],\n",
    "              dtype=np.float32)\n",
    "    v_batch = torch.FloatTensor(d)\n",
    "    vid_positions = np.array([_pad(np.arange(1, x[3].shape[0] + 1), max_vid_len)\n",
    "                            for x in batch], dtype=np.int)\n",
    "    vid_positions = torch.LongTensor(vid_positions)\n",
    "    \n",
    "    ####################\n",
    "    # text positions\n",
    "    text_positions = np.array([_pad(np.arange(1, len(x[0]) + 1), max_input_len)\n",
    "                            for x in batch], dtype=np.int)\n",
    "    text_positions = torch.LongTensor(text_positions)\n",
    "  \n",
    "    max_decoder_target_len = max_target_len // r // downsample_step\n",
    "\n",
    "    # frame positions\n",
    "    s, e = 1, max_decoder_target_len + 1\n",
    "    # if b_pad > 0:\n",
    "    #    s, e = s - 1, e - 1\n",
    "    # NOTE: needs clone to supress RuntimeError in dataloarder...\n",
    "    # ref: https://github.com/pytorch/pytorch/issues/10756\n",
    "    frame_positions = torch.arange(s, e).long().unsqueeze(0).expand(\n",
    "     len(batch), max_decoder_target_len).clone()\n",
    "\n",
    "    # done flags\n",
    "    done = np.array([_pad(np.zeros(len(x[1]) // r // downsample_step - 1),\n",
    "                       max_decoder_target_len, constant_values=1)\n",
    "                  for x in batch])\n",
    "    done = torch.FloatTensor(done).unsqueeze(-1)\n",
    "  \n",
    "    if multi_speaker:\n",
    "        speaker_ids = torch.LongTensor([x[3] for x in batch])\n",
    "    else:\n",
    "        speaker_ids = None\n",
    "  \n",
    "    return x_batch, (input_lengths, vid_input_lengths), mel_batch, y_batch, v_batch, \\\n",
    "        (text_positions, frame_positions, vid_positions), done, target_lengths, speaker_ids\n",
    "\n",
    "\n",
    "def time_string():\n",
    "    return datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "\n",
    "def save_alignment(path, attn, title):\n",
    "    plot_alignment(attn.T, path, title, info=\"{}, {}, step={}\".format(\n",
    "        hparams.builder, time_string(), global_step))\n",
    "\n",
    "\n",
    "def prepare_spec_image(spectrogram):\n",
    "    # [0, 1]\n",
    "    spectrogram = (spectrogram - np.min(spectrogram)) / (np.max(spectrogram) - np.min(spectrogram))\n",
    "    spectrogram = np.flip(spectrogram, axis=1)  # flip against freq axis\n",
    "    return np.uint8(cm.magma(spectrogram.T) * 255)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(global_step, writer, device, model, checkpoint_dir, ismultispeaker):\n",
    "    # harded coded\n",
    "    texts = [\n",
    "        \"Scientists at the CERN laboratory say they have discovered a new particle.\",\n",
    "        \"There's a way to measure the acute emotional intelligence that has never gone out of style.\",\n",
    "        \"President Trump met with other leaders at the Group of 20 conference.\",\n",
    "        \"Generative adversarial network or variational auto-encoder.\",\n",
    "        \"Please call Stella.\",\n",
    "        \"Some have accepted this as a miracle without any physical explanation.\",\n",
    "    ]\n",
    "    import synthesis\n",
    "    synthesis._frontend = _frontend\n",
    "\n",
    "    eval_output_dir = join(checkpoint_dir, \"eval\")\n",
    "    os.makedirs(eval_output_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare model for evaluation\n",
    "    model_eval = build_model().to(device)\n",
    "    model_eval.load_state_dict(model.state_dict())\n",
    "\n",
    "    # hard coded\n",
    "    speaker_ids = [0, 1, 10] if ismultispeaker else [None]\n",
    "    for speaker_id in speaker_ids:\n",
    "        speaker_str = \"multispeaker{}\".format(speaker_id) if speaker_id is not None else \"single\"\n",
    "\n",
    "        for idx, text in enumerate(texts):\n",
    "            signal, alignment, _, mel = synthesis.tts(\n",
    "                model_eval, text, p=0, speaker_id=speaker_id, fast=True)\n",
    "            signal /= np.max(np.abs(signal))\n",
    "\n",
    "            # Alignment\n",
    "            path = join(eval_output_dir, \"step{:09d}_text{}_{}_alignment.png\".format(\n",
    "                global_step, idx, speaker_str))\n",
    "            save_alignment(path, alignment)\n",
    "            tag = \"eval_averaged_alignment_{}_{}\".format(idx, speaker_str)\n",
    "            # try:\n",
    "            #     writer.add_image(tag, np.uint8(cm.viridis(np.flip(alignment, 1).T) * 255), global_step)\n",
    "            # except Exception as e:\n",
    "            #     warn(str(e))\n",
    "\n",
    "            # # Mel\n",
    "            # try:\n",
    "            #     writer.add_image(\"(Eval) Predicted mel spectrogram text{}_{}\".format(idx, speaker_str),\n",
    "            #                      prepare_spec_image(mel), global_step)\n",
    "            # except Exception as e:\n",
    "            #     warn(str(e))\n",
    "\n",
    "            # Audio\n",
    "            path = join(eval_output_dir, \"step{:09d}_text{}_{}_predicted.wav\".format(\n",
    "                global_step, idx, speaker_str))\n",
    "            audio.save_wav(signal, path)\n",
    "\n",
    "            # try:\n",
    "            #     writer.add_audio(\"(Eval) Predicted audio signal {}_{}\".format(idx, speaker_str),\n",
    "            #                      signal, global_step, sample_rate=hparams.sample_rate)\n",
    "            # except Exception as e:\n",
    "            #     warn(str(e))\n",
    "            #     pass\n",
    "\n",
    "\n",
    "def save_states(global_step, writer, mel_outputs, linear_outputs, attn, attn_f, mel, y,\n",
    "                input_lengths, checkpoint_dir=None):\n",
    "    print(\"Save intermediate states at step {}\".format(global_step))\n",
    "\n",
    "    # idx = np.random.randint(0, len(input_lengths))\n",
    "    idx = min(1, len(input_lengths) - 1)\n",
    "    input_length = input_lengths[idx]\n",
    "\n",
    "    # Alignment\n",
    "    # Multi-hop attention\n",
    "    if attn is not None and attn.dim() == 4:\n",
    "        for i, alignment in enumerate(attn):\n",
    "            alignment = alignment[idx].cpu().data.numpy()\n",
    "            tag = \"alignment_layer{}\".format(i + 1)\n",
    "            try:\n",
    "                # writer.add_image(tag, np.uint8(cm.viridis(\n",
    "                #     np.flip(alignment, 1).T) * 255), global_step)\n",
    "                # save files as well for now\n",
    "                alignment_dir = join(\n",
    "                    checkpoint_dir, \"alignment_layer{}\".format(i + 1))\n",
    "                os.makedirs(alignment_dir, exist_ok=True)\n",
    "                path = join(alignment_dir, \"step{:09d}_layer_{}_alignment.png\".format(\n",
    "                    global_step, i + 1))\n",
    "                save_alignment(path, alignment, \"Text\")\n",
    "            except Exception as e:\n",
    "                warn(str(e))\n",
    "\n",
    "        # Save averaged alignment\n",
    "        alignment_dir = join(checkpoint_dir, \"alignment_ave\")\n",
    "        os.makedirs(alignment_dir, exist_ok=True)\n",
    "        path = join(alignment_dir, \"step{:09d}_layer_alignment.png\".format(global_step))\n",
    "        alignment = attn.mean(0)[idx].cpu().data.numpy()\n",
    "        save_alignment(path, alignment, \"Text\")\n",
    "        tag = \"averaged_alignment\"\n",
    "\n",
    "        # try:\n",
    "        #     writer.add_image(tag, np.uint8(cm.viridis(\n",
    "        #         np.flip(alignment, 1).T) * 255), global_step)\n",
    "        # except Exception as e:\n",
    "        #     warn(str(e))\n",
    "\n",
    "    #### Video alignment\n",
    "    if attn_f is not None and attn_f.dim() == 4:\n",
    "        for i, alignment in enumerate(attn_f):\n",
    "            alignment = alignment[idx].cpu().data.numpy()\n",
    "            tag = \"alignment_layer_v{}\".format(i + 1)\n",
    "            try:\n",
    "                # writer.add_image(tag, np.uint8(cm.viridis(\n",
    "                #     np.flip(alignment, 1).T) * 255), global_step)\n",
    "                # save files as well for now\n",
    "                alignment_dir = join(\n",
    "                    checkpoint_dir, \"alignment_layer{}\".format(i + 1))\n",
    "                os.makedirs(alignment_dir, exist_ok=True)\n",
    "                path = join(alignment_dir, \"step{:09d}_layer_{}_alignment_v.png\".format(\n",
    "                    global_step, i + 1))\n",
    "                save_alignment(path, alignment, \"Video\")\n",
    "            except Exception as e:\n",
    "                warn(str(e))\n",
    "\n",
    "        # Save averaged alignment\n",
    "        alignment_dir = join(checkpoint_dir, \"alignment_ave\")\n",
    "        os.makedirs(alignment_dir, exist_ok=True)\n",
    "        path = join(alignment_dir, \"step{:09d}_layer_alignment_v.png\".format(global_step))\n",
    "        alignment = attn_f.mean(0)[idx].cpu().data.numpy()\n",
    "        save_alignment(path, alignment, \"Video\")\n",
    "        tag = \"averaged_alignment_v\"\n",
    "\n",
    "        # try:\n",
    "        #     writer.add_image(tag, np.uint8(cm.viridis(\n",
    "        #         np.flip(alignment, 1).T) * 255), global_step)\n",
    "        # except Exception as e:\n",
    "        #     warn(str(e))\n",
    "\n",
    "    # Predicted mel spectrogram\n",
    "    if mel_outputs is not None:\n",
    "        mel_output = mel_outputs[idx].cpu().data.numpy()\n",
    "        mel_output = prepare_spec_image(audio._denormalize(mel_output))\n",
    "        # try:\n",
    "        #     writer.add_image(\"Predicted mel spectrogram\",\n",
    "        #                      mel_output, global_step)\n",
    "        # except Exception as e:\n",
    "        #     warn(str(e))\n",
    "        #     pass\n",
    "\n",
    "    # Predicted spectrogram\n",
    "    if linear_outputs is not None:\n",
    "        linear_output = linear_outputs[idx].cpu().data.numpy()\n",
    "        spectrogram = prepare_spec_image(audio._denormalize(linear_output))\n",
    "        # try:\n",
    "        #     writer.add_image(\"Predicted linear spectrogram\",\n",
    "        #                      spectrogram, global_step)\n",
    "        # except Exception as e:\n",
    "        #     warn(str(e))\n",
    "        #     pass\n",
    "\n",
    "        # Predicted audio signal\n",
    "        signal = audio.inv_spectrogram(linear_output.T)\n",
    "        signal /= np.max(np.abs(signal))\n",
    "        path = join(checkpoint_dir, \"step{:09d}_predicted.wav\".format(\n",
    "            global_step))\n",
    "        # try:\n",
    "        #     writer.add_audio(\"Predicted audio signal\", signal,\n",
    "        #                      global_step, sample_rate=hparams.sample_rate)\n",
    "        # except Exception as e:\n",
    "        #     warn(str(e))\n",
    "        #     pass\n",
    "        audio.save_wav(signal, path)\n",
    "\n",
    "    # Target mel spectrogram\n",
    "    if mel_outputs is not None:\n",
    "        mel_output = mel[idx].cpu().data.numpy()\n",
    "        mel_output = prepare_spec_image(audio._denormalize(mel_output))\n",
    "        # try:\n",
    "        #     writer.add_image(\"Target mel spectrogram\", mel_output, global_step)\n",
    "        # except Exception as e:\n",
    "        #     warn(str(e))\n",
    "        #     pass\n",
    "\n",
    "    # Target spectrogram\n",
    "    if linear_outputs is not None:\n",
    "        linear_output = y[idx].cpu().data.numpy()\n",
    "        spectrogram = prepare_spec_image(audio._denormalize(linear_output))\n",
    "        # try:\n",
    "        #     writer.add_image(\"Target linear spectrogram\",\n",
    "        #                      spectrogram, global_step)\n",
    "        # except Exception as e:\n",
    "        #     warn(str(e))\n",
    "        #     pass\n",
    "\n",
    "\n",
    "def logit(x, eps=1e-8):\n",
    "    return torch.log(x + eps) - torch.log(1 - x + eps)\n",
    "\n",
    "\n",
    "def masked_mean(y, mask):\n",
    "    # (B, T, D)\n",
    "    mask_ = mask.expand_as(y)\n",
    "    return (y * mask_).sum() / mask_.sum()\n",
    "\n",
    "\n",
    "def spec_loss(y_hat, y, mask, priority_bin=None, priority_w=0):\n",
    "    masked_l1 = MaskedL1Loss()\n",
    "    l1 = nn.L1Loss()\n",
    "\n",
    "    w = hparams.masked_loss_weight\n",
    "\n",
    "    # L1 loss\n",
    "    if w > 0:\n",
    "        assert mask is not None\n",
    "        l1_loss = w * masked_l1(y_hat, y, mask=mask) + (1 - w) * l1(y_hat, y)\n",
    "    else:\n",
    "        assert mask is None\n",
    "        l1_loss = l1(y_hat, y)\n",
    "\n",
    "    # Priority L1 loss\n",
    "    if priority_bin is not None and priority_w > 0:\n",
    "        if w > 0:\n",
    "            priority_loss = w * masked_l1(\n",
    "                y_hat[:, :, :priority_bin], y[:, :, :priority_bin], mask=mask) \\\n",
    "                + (1 - w) * l1(y_hat[:, :, :priority_bin], y[:, :, :priority_bin])\n",
    "        else:\n",
    "            priority_loss = l1(y_hat[:, :, :priority_bin], y[:, :, :priority_bin])\n",
    "        l1_loss = (1 - priority_w) * l1_loss + priority_w * priority_loss\n",
    "\n",
    "    # Binary divergence loss\n",
    "    if hparams.binary_divergence_weight <= 0:\n",
    "        binary_div = y.data.new(1).zero_()\n",
    "    else:\n",
    "        y_hat_logits = logit(y_hat)\n",
    "        z = -y * y_hat_logits + torch.log1p(torch.exp(y_hat_logits))\n",
    "        if w > 0:\n",
    "            binary_div = w * masked_mean(z, mask) + (1 - w) * z.mean()\n",
    "        else:\n",
    "            binary_div = z.mean()\n",
    "\n",
    "    return l1_loss, binary_div\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def guided_attention(N, max_N, T, max_T, g):\n",
    "    W = np.zeros((max_N, max_T), dtype=np.float32)\n",
    "    for n in range(N):\n",
    "        for t in range(T):\n",
    "            W[n, t] = 1 - np.exp(-(n / N - t / T)**2 / (2 * g * g))\n",
    "    return W\n",
    "\n",
    "\n",
    "def guided_attentions(input_lengths, target_lengths, max_target_len, g=0.2):\n",
    "    B = len(input_lengths)\n",
    "    max_input_len = input_lengths.max()\n",
    "    W = np.zeros((B, max_target_len, max_input_len), dtype=np.float32)\n",
    "    for b in range(B):\n",
    "        W[b] = guided_attention(input_lengths[b], max_input_len,\n",
    "                                target_lengths[b], max_target_len, g).T\n",
    "    return W\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, model, data_loader, optimizer, writer,\n",
    "          init_lr=0.002,\n",
    "          checkpoint_dir=None, checkpoint_interval=None, nepochs=None,\n",
    "          clip_thresh=1.0,\n",
    "          train_seq2seq=True, train_postnet=True, batch_size=4):\n",
    "    linear_dim = model.linear_dim\n",
    "    r = hparams.outputs_per_step\n",
    "    downsample_step = hparams.downsample_step\n",
    "    current_lr = init_lr\n",
    "    print(device)\n",
    "    binary_criterion = nn.BCELoss()\n",
    "\n",
    "    assert train_seq2seq or train_postnet\n",
    "\n",
    "    global global_step, global_epoch\n",
    "    while global_epoch < nepochs:\n",
    "        running_loss = 0.\n",
    "        for step, (x, input_audvid_lengths, mel, y, v, positions, done, target_lengths,\n",
    "                   speaker_ids) \\\n",
    "                in tqdm(enumerate(data_loader)):\n",
    "            model.train()\n",
    "            ismultispeaker = speaker_ids is not None\n",
    "            # Learning rate schedule\n",
    "            if hparams.lr_schedule is not None:\n",
    "                lr_schedule_f = getattr(lrschedule, hparams.lr_schedule)\n",
    "                current_lr = lr_schedule_f(\n",
    "                    init_lr, global_step, **hparams.lr_schedule_kwargs)\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = current_lr\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Used for Position encoding\n",
    "            text_positions, frame_positions, vid_positions = positions\n",
    "            input_lengths, video_length = input_audvid_lengths \n",
    "            # Downsample mel spectrogram\n",
    "            if downsample_step > 1:\n",
    "                mel = mel[:, 0::downsample_step, :].contiguous()\n",
    "\n",
    "            # Lengths\n",
    "            input_lengths = input_lengths.long().numpy()\n",
    "            video_length = video_length.long().numpy()\n",
    "            decoder_lengths = target_lengths.long().numpy() // r // downsample_step\n",
    "\n",
    "            max_seq_len = max(input_lengths.max(), decoder_lengths.max())\n",
    "            if max_seq_len >= hparams.max_positions:\n",
    "                raise RuntimeError(\n",
    "                    \"\"\"max_seq_len ({}) >= max_posision ({})\n",
    "Input text or decoder targget length exceeded the maximum length.\n",
    "Please set a larger value for ``max_position`` in hyper parameters.\"\"\".format(\n",
    "                        max_seq_len, hparams.max_positions))\n",
    "\n",
    "            # Transform data to CUDA device\n",
    "            if train_seq2seq:\n",
    "                x = x.to(device)\n",
    "                text_positions = text_positions.to(device)\n",
    "                frame_positions = frame_positions.to(device)\n",
    "            if train_postnet:\n",
    "                y = y.to(device)\n",
    "#             video_length = video_length.to(device)\n",
    "            v = v.to(device)\n",
    "            # print(v.shape)\n",
    "            vid_positions = vid_positions.to(device)\n",
    "            # print(video_length)\n",
    "            mel, done = mel.to(device), done.to(device)\n",
    "            target_lengths = target_lengths.to(device)\n",
    "            speaker_ids = speaker_ids.to(device) if ismultispeaker else None\n",
    "\n",
    "            # Create mask if we use masked loss\n",
    "            if hparams.masked_loss_weight > 0:\n",
    "                # decoder output domain mask\n",
    "                decoder_target_mask = sequence_mask(\n",
    "                    target_lengths // (r * downsample_step),\n",
    "                    max_len=mel.size(1)).unsqueeze(-1)\n",
    "                if downsample_step > 1:\n",
    "                    # spectrogram-domain mask\n",
    "                    target_mask = sequence_mask(\n",
    "                        target_lengths, max_len=y.size(1)).unsqueeze(-1)\n",
    "                else:\n",
    "                    target_mask = decoder_target_mask\n",
    "                # shift mask\n",
    "                decoder_target_mask = decoder_target_mask[:, r:, :]\n",
    "                target_mask = target_mask[:, r:, :]\n",
    "            else:\n",
    "                decoder_target_mask, target_mask = None, None\n",
    "\n",
    "            # Apply model\n",
    "            if train_seq2seq and train_postnet:\n",
    "#                 print('l')\n",
    "                mel_outputs, linear_outputs, attn, done_hat, attn_f = model(\n",
    "                    x, mel, speaker_ids=speaker_ids,\n",
    "                    text_positions=text_positions, frame_positions=frame_positions, vid_positions=vid_positions,\n",
    "                    input_lengths=input_lengths, faces=v, video_lengths=video_length, batch_size=batch_size)\n",
    "            elif train_seq2seq:\n",
    "                assert speaker_ids is None\n",
    "                mel_outputs, attn, done_hat, _ = model.seq2seq(\n",
    "                    x, mel,\n",
    "                    text_positions=text_positions, frame_positions=frame_positions,\n",
    "                    input_lengths=input_lengths)\n",
    "                # reshape\n",
    "                mel_outputs = mel_outputs.view(len(mel), -1, mel.size(-1))\n",
    "                linear_outputs = None\n",
    "            elif train_postnet:\n",
    "                assert speaker_ids is None\n",
    "                linear_outputs = model.postnet(mel)\n",
    "                mel_outputs, attn, done_hat = None, None, None\n",
    "\n",
    "            # Losses\n",
    "            w = hparams.binary_divergence_weight\n",
    "\n",
    "            # mel:\n",
    "            if train_seq2seq:\n",
    "                mel_l1_loss, mel_binary_div = spec_loss(\n",
    "                    mel_outputs[:, :-r, :], mel[:, r:, :], decoder_target_mask)\n",
    "                mel_loss = (1 - w) * mel_l1_loss + w * mel_binary_div\n",
    "\n",
    "            # done:\n",
    "            if train_seq2seq:\n",
    "                done_loss = binary_criterion(done_hat, done)\n",
    "\n",
    "            # linear:\n",
    "            if train_postnet:\n",
    "                n_priority_freq = int(hparams.priority_freq / (hparams.sample_rate * 0.5) * linear_dim)\n",
    "                linear_l1_loss, linear_binary_div = spec_loss(\n",
    "                    linear_outputs[:, :-r, :], y[:, r:, :], target_mask,\n",
    "                    priority_bin=n_priority_freq,\n",
    "                    priority_w=hparams.priority_freq_weight)\n",
    "                linear_loss = (1 - w) * linear_l1_loss + w * linear_binary_div\n",
    "\n",
    "            # Combine losses\n",
    "            if train_seq2seq and train_postnet:\n",
    "                loss = mel_loss + linear_loss + done_loss\n",
    "            elif train_seq2seq:\n",
    "                loss = mel_loss + done_loss\n",
    "            elif train_postnet:\n",
    "                loss = linear_loss\n",
    "\n",
    "            # attention\n",
    "            # print(input_lengths.shape)\n",
    "            # print(video_length.shape)\n",
    "            if train_seq2seq and hparams.use_guided_attention:\n",
    "                soft_mask = guided_attentions(input_lengths, decoder_lengths,\n",
    "                                              attn.size(-2),\n",
    "                                              g=hparams.guided_attention_sigma)\n",
    "                soft_mask = torch.from_numpy(soft_mask).to(device)\n",
    "                attn_loss = (attn * soft_mask).mean()\n",
    "                loss += attn_loss\n",
    "\n",
    "                soft_mask_f = guided_attentions(video_length, decoder_lengths,\n",
    "                                              attn_f.size(-2),\n",
    "                                              g=hparams.guided_attention_sigma)\n",
    "                # print('error check')\n",
    "                # print(decoder_lengths)\n",
    "                # print(attn_f.shape,soft_mask_f.shape)\n",
    "                soft_mask_f = torch.from_numpy(soft_mask_f).to(device)\n",
    "                attn_loss_f = (attn_f * soft_mask_f).mean()\n",
    "                loss += attn_loss_f\n",
    "\n",
    "\n",
    "            if global_step > 0 and global_step % checkpoint_interval == 0:\n",
    "                save_states(\n",
    "                    global_step, writer, mel_outputs, linear_outputs, attn, attn_f,\n",
    "                    mel, y, input_lengths, checkpoint_dir)\n",
    "                save_checkpoint(\n",
    "                    model, optimizer, global_step, checkpoint_dir, global_epoch,\n",
    "                    train_seq2seq, train_postnet)\n",
    "\n",
    "            # if global_step > 0 and global_step % hparams.eval_interval == 0:\n",
    "            #     eval_model(global_step, writer, device, model,\n",
    "            #                checkpoint_dir, ismultispeaker)\n",
    "\n",
    "            # Update\n",
    "            loss.backward()\n",
    "            if clip_thresh > 0:\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    model.get_trainable_parameters(), clip_thresh)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Logs\n",
    "            # writer.add_scalar(\"loss\", float(loss.item()), global_step)\n",
    "            # if train_seq2seq:\n",
    "            #     writer.add_scalar(\"done_loss\", float(done_loss.item()), global_step)\n",
    "            #     writer.add_scalar(\"mel loss\", float(mel_loss.item()), global_step)\n",
    "            #     writer.add_scalar(\"mel_l1_loss\", float(mel_l1_loss.item()), global_step)\n",
    "            #     writer.add_scalar(\"mel_binary_div_loss\", float(mel_binary_div.item()), global_step)\n",
    "            # if train_postnet:\n",
    "            #     writer.add_scalar(\"linear_loss\", float(linear_loss.item()), global_step)\n",
    "            #     writer.add_scalar(\"linear_l1_loss\", float(linear_l1_loss.item()), global_step)\n",
    "            #     writer.add_scalar(\"linear_binary_div_loss\", float(linear_binary_div.item()), global_step)\n",
    "            # if train_seq2seq and hparams.use_guided_attention:\n",
    "            #     writer.add_scalar(\"attn_loss\", float(attn_loss.item()), global_step)\n",
    "            # if clip_thresh > 0:\n",
    "            #     writer.add_scalar(\"gradient norm\", grad_norm, global_step)\n",
    "            # writer.add_scalar(\"learning rate\", current_lr, global_step)\n",
    "\n",
    "            global_step += 1\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        averaged_loss = running_loss / (len(data_loader))\n",
    "        # writer.add_scalar(\"loss (per epoch)\", averaged_loss, global_epoch)\n",
    "        print(\"Loss: {}\".format(running_loss / (len(data_loader))))\n",
    "\n",
    "        global_epoch += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = getattr(builder, hparams.builder)(\n",
    "        n_speakers=hparams.n_speakers,\n",
    "        speaker_embed_dim=hparams.speaker_embed_dim,\n",
    "        n_vocab=_frontend.n_vocab,\n",
    "        embed_dim=hparams.text_embed_dim,\n",
    "        mel_dim=hparams.num_mels,\n",
    "        linear_dim=hparams.fft_size // 2 + 1,\n",
    "        r=hparams.outputs_per_step,\n",
    "        downsample_step=hparams.downsample_step,\n",
    "        padding_idx=hparams.padding_idx,\n",
    "        dropout=hparams.dropout,\n",
    "        kernel_size=hparams.kernel_size,\n",
    "        encoder_channels=hparams.encoder_channels,\n",
    "        decoder_channels=hparams.decoder_channels,\n",
    "        converter_channels=hparams.converter_channels,\n",
    "        use_memory_mask=hparams.use_memory_mask,\n",
    "        trainable_positional_encodings=hparams.trainable_positional_encodings,\n",
    "        force_monotonic_attention=hparams.force_monotonic_attention,\n",
    "        use_decoder_state_for_postnet_input=hparams.use_decoder_state_for_postnet_input,\n",
    "        max_positions=hparams.max_positions,\n",
    "        speaker_embedding_weight_std=hparams.speaker_embedding_weight_std,\n",
    "        freeze_embedding=hparams.freeze_embedding,\n",
    "        window_ahead=hparams.window_ahead,\n",
    "        window_backward=hparams.window_backward,\n",
    "        key_projection=hparams.key_projection,\n",
    "        value_projection=hparams.value_projection,\n",
    "    )\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _load_embedding(path, model):\n",
    "    state = _load(path)[\"state_dict\"]\n",
    "    key = \"seq2seq.encoder.embed_tokens.weight\"\n",
    "    model.seq2seq.encoder.embed_tokens.weight.data = state[key]\n",
    "\n",
    "# https://discuss.pytorch.org/t/how-to-load-part-of-pre-trained-model/1113/3\n",
    "\n",
    "\n",
    "def restore_parts(path, model):\n",
    "    print(\"Restore part of the model from: {}\".format(path))\n",
    "    state = _load(path)[\"state_dict\"]\n",
    "    model_dict = model.state_dict()\n",
    "    valid_state_dict = {k: v for k, v in state.items() if k in model_dict}\n",
    "\n",
    "    try:\n",
    "        model_dict.update(valid_state_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "    except RuntimeError as e:\n",
    "        # there should be invalid size of weight(s), so load them per parameter\n",
    "        print(str(e))\n",
    "        model_dict = model.state_dict()\n",
    "        for k, v in valid_state_dict.items():\n",
    "            model_dict[k] = v\n",
    "            try:\n",
    "                model.load_state_dict(model_dict)\n",
    "            except RuntimeError as e:\n",
    "                print(str(e))\n",
    "                warn(\"{}: may contain invalid size of weight. skipping...\".format(k))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_frontend = getattr(frontend, hparams.frontend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root= '/ssd_scratch/cvit/anchit/training/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_id=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = FileSourceDataset(TextDataSource(data_root, speaker_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mel = FileSourceDataset(MelSpecDataSource(data_root, speaker_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = FileSourceDataset(LinearSpecDataSource(data_root, speaker_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = FileSourceDataset(VisualDataSource(data_root, speaker_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_lengths = Mel.file_data_source.frame_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = PartialyRandomizedSimilarTimeLengthSampler(\n",
    "         frame_lengths, batch_size=hparams.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PyTorchDataset(X, Mel, Y, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = data_utils.DataLoader(\n",
    "         dataset, batch_size=hparams.batch_size,\n",
    "         num_workers=hparams.num_workers, sampler=sampler,\n",
    "         collate_fn=collate_fn, pin_memory=hparams.pin_memory, drop_last=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "# print(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " optimizer = optim.Adam(model.get_trainable_parameters(),\n",
    "                           lr=hparams.initial_learning_rate, betas=(\n",
    "        hparams.adam_beta1, hparams.adam_beta2),\n",
    "        eps=hparams.adam_eps, weight_decay=hparams.weight_decay,\n",
    "        amsgrad=hparams.amsgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_event_path = './'\n",
    "writer = SummaryWriter(log_event_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_dim = model.linear_dim\n",
    "r = hparams.outputs_per_step\n",
    "downsample_step = hparams.downsample_step\n",
    "current_lr = 0.002\n",
    "checkpoint_interval=None\n",
    "checkpoint_dir=None\n",
    "nepochs=None\n",
    "clip_thresh=1.0\n",
    "train_seq2seq=True\n",
    "train_postnet=True\n",
    "binary_criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:01, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepvoice3 mai\n",
      "torch.Size([4, 118, 256])\n",
      "torch.Size([4, 256, 131])\n",
      "torch.Size([4, 118, 256])\n",
      "latest edits\n",
      "torch.Size([4, 118, 512])\n",
      "yaha valueka projection\n",
      "yaha keys projection\n",
      "deepvoice3 mai\n",
      "torch.Size([4, 118, 512])\n",
      "torch.Size([4, 512, 98])\n",
      "torch.Size([4, 118, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (512) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-89d5f83a814b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mnepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mclip_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_thresh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m           train_seq2seq=train_seq2seq, train_postnet=train_postnet)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-7709900b9448>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(device, model, data_loader, optimizer, writer, init_lr, checkpoint_dir, checkpoint_interval, nepochs, clip_thresh, train_seq2seq, train_postnet, batch_size)\u001b[0m\n\u001b[1;32m     90\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeaker_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspeaker_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mtext_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframe_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvid_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvid_positions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     input_lengths=input_lengths, faces=v, video_lengths=video_length, batch_size=batch_size)\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtrain_seq2seq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mspeaker_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deepvoice3_pytorch/deepvoice3_pytorch/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text_sequences, mel_targets, speaker_ids, text_positions, frame_positions, vid_positions, input_lengths, faces, video_lengths, batch_size)\u001b[0m\n\u001b[1;32m     77\u001b[0m         mel_outputs, alignments, done, decoder_states, alignments_f = self.seq2seq(\n\u001b[1;32m     78\u001b[0m             \u001b[0mtext_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeaker_embed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             text_positions, frame_positions, vid_positions, input_lengths, video_lengths, faces, batch_size)\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# Reshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deepvoice3_pytorch/deepvoice3_pytorch/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text_sequences, mel_targets, speaker_embed, text_positions, frame_positions, vid_positions, input_lengths, video_lengths, faces, batch_size)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_encoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mtext_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframe_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvid_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvid_positions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             speaker_embed=speaker_embed, lengths=input_lengths, video_lengths=video_lengths)\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;31m# print(alignments.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# print(alignments_f.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deepvoice3_pytorch/deepvoice3_pytorch/nyanko.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_out, face_encoder_out, inputs, text_positions, frame_positions, vid_positions, speaker_embed, lengths, video_lengths)\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverall_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;31m#               R, alignments = self.attention(x, (keys, values), mask=mask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m                 \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malignments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_cat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverall_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;31m# print(\"R\", R.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deepvoice3_pytorch/deepvoice3_pytorch/deepvoice3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, encoder_out, mask, last_attended)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_projection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (512) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "    train_seq2seq=True\n",
    "    train_postnet=True\n",
    "    train(device, model, data_loader, optimizer, writer,\n",
    "              init_lr=hparams.initial_learning_rate,\n",
    "              checkpoint_dir=checkpoint_dir,\n",
    "              checkpoint_interval=hparams.checkpoint_interval,\n",
    "              nepochs=hparams.nepochs,\n",
    "              clip_thresh=hparams.clip_thresh,\n",
    "              train_seq2seq=train_seq2seq, train_postnet=train_postnet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
