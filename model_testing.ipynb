{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docopt import docopt\n",
    "\n",
    "import sys\n",
    "import gc\n",
    "import platform\n",
    "from os.path import dirname, join\n",
    "from tqdm import tqdm, trange\n",
    "from datetime import datetime\n",
    "\n",
    "# The deepvoice3 model\n",
    "from deepvoice3_pytorch import frontend, builder\n",
    "import audio\n",
    "import lrschedule\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils import data as data_utils\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import numpy as np\n",
    "from numba import jit\n",
    "\n",
    "from nnmnkwii.datasets import FileSourceDataset, FileDataSource\n",
    "from os.path import join, expanduser\n",
    "import random\n",
    "\n",
    "import librosa.display\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "from matplotlib import cm\n",
    "from warnings import warn\n",
    "import cv2\n",
    "from hparams import hparams, hparams_debug_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedL1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedL1Loss, self).__init__()\n",
    "        self.criterion = nn.L1Loss(reduction=\"sum\")\n",
    "\n",
    "    def forward(self, input, target, lengths=None, mask=None, max_len=None):\n",
    "        if lengths is None and mask is None:\n",
    "            raise RuntimeError(\"Should provide either lengths or mask\")\n",
    "\n",
    "        # (B, T, 1)\n",
    "        if mask is None:\n",
    "            mask = sequence_mask(lengths, max_len).unsqueeze(-1)\n",
    "\n",
    "        # (B, T, D)\n",
    "        mask_ = mask.expand_as(input)\n",
    "        loss = self.criterion(input * mask_, target * mask_)\n",
    "        return loss / mask_.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "global_epoch = 0\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    cudnn.benchmark = False\n",
    "\n",
    "_frontend = None  # to be set later\n",
    "\n",
    "\n",
    "def _pad(seq, max_len, constant_values=0):\n",
    "    return np.pad(seq, (0, max_len - len(seq)),\n",
    "                  mode='constant', constant_values=constant_values)\n",
    "\n",
    "\n",
    "def _pad_2d(x, max_len, b_pad=0):\n",
    "    x = np.pad(x, [(b_pad, max_len - len(x) - b_pad), (0, 0)],\n",
    "               mode=\"constant\", constant_values=0)\n",
    "    return x\n",
    "\n",
    "def _pad_4d(x, max_len):\n",
    "    idx = x.shape[0]\n",
    "    y = np.zeros((max_len,48,96,3))\n",
    "    y[:idx] = x\n",
    "    return y\n",
    "    \n",
    "    \n",
    "def plot_alignment(alignment, path, info=None):\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(\n",
    "        alignment,\n",
    "        aspect='auto',\n",
    "        origin='lower',\n",
    "        interpolation='none')\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    xlabel = 'Decoder timestep'\n",
    "    if info is not None:\n",
    "        xlabel += '\\n\\n' + info\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Encoder timestep')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, format='png')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(x, eps=1e-8):\n",
    "    return torch.log(x + eps) - torch.log(1 - x + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataSource(FileDataSource):    \n",
    "    def __init__(self, data_root, speaker_id=None):        \n",
    "        self.data_root = data_root        \n",
    "        self.speaker_ids = None\n",
    "        self.multi_speaker = False\n",
    "        # If not None, filter by speaker_id\n",
    "        self.speaker_id = speaker_id\n",
    "\n",
    "    def collect_files(self):\n",
    "        meta = join(self.data_root, \"train.txt\")\n",
    "        with open(meta, \"rb\") as f:\n",
    "            lines = f.readlines()\n",
    "        l = lines[0].decode(\"utf-8\").split(\"|\")\n",
    "        assert len(l) == 5 or len(l) == 6\n",
    "        self.multi_speaker = len(l) == 6\n",
    "        texts = list(map(lambda l: l.decode(\"utf-8\").split(\"|\")[3], lines))\n",
    "        if self.multi_speaker:\n",
    "            speaker_ids = list(map(lambda l: int(l.decode(\"utf-8\").split(\"|\")[-1]), lines))\n",
    "            # Filter by speaker_id\n",
    "            # using multi-speaker dataset as a single speaker dataset\n",
    "            if self.speaker_id is not None:\n",
    "                indices = np.array(speaker_ids) == self.speaker_id\n",
    "                texts = list(np.array(texts)[indices])\n",
    "                self.multi_speaker = False\n",
    "                return texts\n",
    "\n",
    "            return texts, speaker_ids\n",
    "        else:\n",
    "            return texts\n",
    "\n",
    "    def collect_features(self, *args):\n",
    "        if self.multi_speaker:\n",
    "            text, speaker_id = args\n",
    "        else:\n",
    "            text = args[0]\n",
    "        global _frontend\n",
    "        if _frontend is None:\n",
    "            _frontend = getattr(frontend, hparams.frontend)\n",
    "        seq = _frontend.text_to_sequence(text, p=hparams.replace_pronunciation_prob)\n",
    "\n",
    "        if platform.system() == \"Windows\":\n",
    "            if hasattr(hparams, 'gc_probability'):\n",
    "                _frontend = None  # memory leaking prevention in Windows\n",
    "                if np.random.rand() < hparams.gc_probability:\n",
    "                    gc.collect()  # garbage collection enforced\n",
    "                    print(\"GC done\")\n",
    "\n",
    "        if self.multi_speaker:\n",
    "            return np.asarray(seq, dtype=np.int32), int(speaker_id)\n",
    "        else:\n",
    "            return np.asarray(seq, dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _NPYDataSource(FileDataSource):\n",
    "     def __init__(self, data_root, col, speaker_id=None):\n",
    "         self.data_root = data_root\n",
    "         self.col = col\n",
    "         self.frame_lengths = []\n",
    "         self.speaker_id = speaker_id\n",
    " \n",
    "     def collect_files(self):\n",
    "         meta = join(self.data_root, \"train.txt\")\n",
    "         with open(meta, \"rb\") as f:\n",
    "             lines = f.readlines()\n",
    "         l = lines[0].decode(\"utf-8\").split(\"|\")\n",
    "         assert len(l) == 5 or len(l) == 6\n",
    "         multi_speaker = len(l) == 6\n",
    "         self.frame_lengths = list(\n",
    "             map(lambda l: int(l.decode(\"utf-8\").split(\"|\")[2]), lines))\n",
    " \n",
    "         paths = list(map(lambda l: l.decode(\"utf-8\").split(\"|\")[self.col], lines))\n",
    "         paths = list(map(lambda f: join(self.data_root, f), paths))\n",
    " \n",
    "         if multi_speaker and self.speaker_id is not None:\n",
    "             speaker_ids = list(map(lambda l: int(l.decode(\"utf-8\").split(\"|\")[-1]), lines))\n",
    "             # Filter by speaker_id\n",
    "             # using multi-speaker dataset as a single speaker dataset\n",
    "             indices = np.array(speaker_ids) == self.speaker_id\n",
    "             paths = list(np.array(paths)[indices])\n",
    "             self.frame_lengths = list(np.array(self.frame_lengths)[indices])\n",
    "             # aha, need to cast numpy.int64 to int\n",
    "             self.frame_lengths = list(map(int, self.frame_lengths))\n",
    " \n",
    "         return paths\n",
    " \n",
    "     def collect_features(self, path):\n",
    "         return np.load(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelSpecDataSource(_NPYDataSource):\n",
    "    def __init__(self, data_root, speaker_id=None):\n",
    "        super(MelSpecDataSource, self).__init__(data_root, 1, speaker_id)\n",
    " \n",
    " \n",
    "class LinearSpecDataSource(_NPYDataSource):\n",
    "    def __init__(self, data_root, speaker_id=None):\n",
    "        super(LinearSpecDataSource, self).__init__(data_root, 0, speaker_id)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mean(y, mask):\n",
    "    # (B, T, D)\n",
    "    mask_ = mask.expand_as(y)\n",
    "    return (y * mask_).sum() / mask_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialyRandomizedSimilarTimeLengthSampler(Sampler):\n",
    "     \"\"\"Partially randmoized sampler\n",
    " \n",
    "     1. Sort by lengths\n",
    "     2. Pick a small patch and randomize it\n",
    "     3. Permutate mini-batchs\n",
    "     \"\"\"\n",
    " \n",
    "     def __init__(self, lengths, batch_size=16, batch_group_size=None,\n",
    "                  permutate=True):\n",
    "         self.lengths, self.sorted_indices = torch.sort(torch.LongTensor(lengths))\n",
    "         self.batch_size = batch_size\n",
    "         if batch_group_size is None:\n",
    "             batch_group_size = min(batch_size * 32, len(self.lengths))\n",
    "             if batch_group_size % batch_size != 0:\n",
    "                 batch_group_size -= batch_group_size % batch_size\n",
    " \n",
    "         self.batch_group_size = batch_group_size\n",
    "         assert batch_group_size % batch_size == 0\n",
    "         self.permutate = permutate\n",
    " \n",
    "     def __iter__(self):\n",
    "         indices = self.sorted_indices.clone()\n",
    "         batch_group_size = self.batch_group_size\n",
    "         s, e = 0, 0\n",
    "         for i in range(len(indices) // batch_group_size):\n",
    "             s = i * batch_group_size\n",
    "             e = s + batch_group_size\n",
    "             random.shuffle(indices[s:e])\n",
    " \n",
    "         # Permutate batches\n",
    "         if self.permutate:\n",
    "             perm = np.arange(len(indices[:e]) // self.batch_size)\n",
    "             random.shuffle(perm)\n",
    "             indices[:e] = indices[:e].view(-1, self.batch_size)[perm, :].view(-1)\n",
    " \n",
    "         # Handle last elements\n",
    "         s += batch_group_size\n",
    "         if s < len(indices):\n",
    "             random.shuffle(indices[s:])\n",
    " \n",
    "         return iter(indices)\n",
    " \n",
    "     def __len__(self):\n",
    "         return len(self.sorted_indices)\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchDataset(object):\n",
    "     def __init__(self, X, Mel, Y, V):\n",
    "            self.X = X\n",
    "            self.Mel = Mel\n",
    "            self.Y = Y\n",
    "            self.V = V\n",
    "            # alias\n",
    "            self.multi_speaker = X.file_data_source.multi_speaker\n",
    " \n",
    "     def __getitem__(self, idx):\n",
    "        if self.multi_speaker:\n",
    "            text, speaker_id = self.X[idx]\n",
    "            return text, self.Mel[idx], self.Y[idx], speaker_id\n",
    "        else:\n",
    "            return self.X[idx], self.Mel[idx], self.Y[idx], self.V[idx]\n",
    " \n",
    "     def __len__(self):\n",
    "        return len(self.X)\n",
    " \n",
    " \n",
    "def sequence_mask(sequence_length, max_len=None):\n",
    "        if max_len is None:\n",
    "            max_len = sequence_length.data.max()\n",
    "        batch_size = sequence_length.size(0)\n",
    "        seq_range = torch.arange(0, max_len).long()\n",
    "        seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "        if sequence_length.is_cuda:\n",
    "            seq_range_expand = seq_range_expand.cuda()\n",
    "            seq_length_expand = sequence_length.unsqueeze(1) \\\n",
    "            .expand_as(seq_range_expand)\n",
    "        return (seq_range_expand < seq_length_expand).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Create batch\"\"\"\n",
    "    r = hparams.outputs_per_step\n",
    "    downsample_step = hparams.downsample_step\n",
    "    multi_speaker = len(batch[0]) == 5\n",
    "\n",
    "    # Lengths\n",
    "    input_lengths = [len(x[0]) for x in batch]\n",
    "    max_input_len = max(input_lengths)\n",
    "\n",
    "    target_lengths = [len(x[1]) for x in batch]\n",
    "\n",
    "    max_target_len = max(target_lengths)\n",
    "    if max_target_len % r != 0:\n",
    "        max_target_len += r - max_target_len % r\n",
    "        assert max_target_len % r == 0\n",
    "    if max_target_len % downsample_step != 0:\n",
    "        max_target_len += downsample_step - max_target_len % downsample_step\n",
    "        assert max_target_len % downsample_step == 0\n",
    "  \n",
    "      # Set 0 for zero beginning padding\n",
    "      # imitates initial decoder states\n",
    "    b_pad = r\n",
    "    max_target_len += b_pad * downsample_step\n",
    "\n",
    "    a = np.array([_pad(x[0], max_input_len) for x in batch], dtype=np.int)\n",
    "    x_batch = torch.LongTensor(a)\n",
    "\n",
    "    input_lengths = torch.LongTensor(input_lengths)\n",
    "    target_lengths = torch.LongTensor(target_lengths)\n",
    "\n",
    "    b = np.array([_pad_2d(x[1], max_target_len, b_pad=b_pad) for x in batch],\n",
    "              dtype=np.float32)\n",
    "    mel_batch = torch.FloatTensor(b)\n",
    "\n",
    "    c = np.array([_pad_2d(x[2], max_target_len, b_pad=b_pad) for x in batch],\n",
    "              dtype=np.float32)\n",
    "    y_batch = torch.FloatTensor(c)\n",
    "    \n",
    "    ## VIDEO ADDITION ##\n",
    "    vid_input_lengths = [x[3].shape[0] for x in batch]\n",
    "    vid_input_lengths = torch.LongTensor(vid_input_lengths)\n",
    "    max_vid_len = max(vid_input_lengths)\n",
    "    d = np.array([_pad_4d(x[3], 248) for x in batch],\n",
    "              dtype=np.float32)\n",
    "    v_batch = torch.FloatTensor(d)\n",
    "    \n",
    "    ####################\n",
    "    # text positions\n",
    "    text_positions = np.array([_pad(np.arange(1, len(x[0]) + 1), max_input_len)\n",
    "                            for x in batch], dtype=np.int)\n",
    "    text_positions = torch.LongTensor(text_positions)\n",
    "  \n",
    "    max_decoder_target_len = max_target_len // r // downsample_step\n",
    "\n",
    "    # frame positions\n",
    "    s, e = 1, max_decoder_target_len + 1\n",
    "    # if b_pad > 0:\n",
    "    #    s, e = s - 1, e - 1\n",
    "    # NOTE: needs clone to supress RuntimeError in dataloarder...\n",
    "    # ref: https://github.com/pytorch/pytorch/issues/10756\n",
    "    frame_positions = torch.arange(s, e).long().unsqueeze(0).expand(\n",
    "     len(batch), max_decoder_target_len).clone()\n",
    "\n",
    "    # done flags\n",
    "    done = np.array([_pad(np.zeros(len(x[1]) // r // downsample_step - 1),\n",
    "                       max_decoder_target_len, constant_values=1)\n",
    "                  for x in batch])\n",
    "    done = torch.FloatTensor(done).unsqueeze(-1)\n",
    "  \n",
    "    if multi_speaker:\n",
    "        speaker_ids = torch.LongTensor([x[3] for x in batch])\n",
    "    else:\n",
    "        speaker_ids = None\n",
    "  \n",
    "    return x_batch, (input_lengths, vid_input_lengths), mel_batch, y_batch, v_batch, \\\n",
    "        (text_positions, frame_positions), done, target_lengths, speaker_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualDataSource(FileDataSource):\n",
    "    def __init__(self, data_root, speaker_id=None):\n",
    "        self.data_root = data_root \n",
    "        self.speaker_ids = None\n",
    "        self.multi_speaker = False\n",
    "        # If not None, filter by speaker_id\n",
    "        self.speaker_id = speaker_id\n",
    "\n",
    "    def collect_files(self):\n",
    "        meta = join(self.data_root, \"train.txt\")\n",
    "        with open(meta, \"rb\") as f:\n",
    "            lines = f.readlines()\n",
    "        l = lines[0].decode(\"utf-8\").split(\"|\")\n",
    "        assert len(l) == 5 or len(l) == 6\n",
    "        self.multi_speaker = len(l) == 6\n",
    "        video_file = list(map(lambda l: l.decode(\"utf-8\").split(\"|\")[4].strip(), lines))\n",
    "        if self.multi_speaker:\n",
    "            speaker_ids = list(map(lambda l: int(l.decode(\"utf-8\").split(\"|\")[-1]), lines))\n",
    "            # Filter by speaker_id\n",
    "            # using multi-speaker dataset as a single speaker dataset\n",
    "            if self.speaker_id is not None:\n",
    "                indices = np.array(speaker_ids) == self.speaker_id\n",
    "                texts = list(np.array(texts)[indices])\n",
    "                self.multi_speaker = False\n",
    "                return texts\n",
    "\n",
    "            return video_file, speaker_ids\n",
    "        else:\n",
    "            return video_file\n",
    "\n",
    "    def collect_features(self, path):\n",
    "#             add wavs to path and then read the video files\n",
    "#         path = path = './wavs/'\n",
    "        path = os.path.join(self.data_root, path)\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frameWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frameHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "        video = np.empty((frameCount, frameHeight, frameWidth, 3), np.dtype('uint8'))\n",
    "\n",
    "        fc = 0\n",
    "        ret = True\n",
    "\n",
    "        while (fc < frameCount  and ret):\n",
    "            ret, video[fc] = cap.read()\n",
    "            fc += 1\n",
    "        cap.release()\n",
    "        \n",
    "        return np.array(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_frontend = getattr(frontend, hparams.frontend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root= '/ssd_scratch/cvit/anchit/training/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=os.listdir(data_root)\n",
    "# print(f[1])\n",
    "# path = join(data_root,V.collected_files[4370][0])\n",
    "# path_o = join(data_root,f[1])\n",
    "# if path == path_o:\n",
    "#     print('lm')\n",
    "# print(path)\n",
    "# cap = cv2.VideoCapture(path)\n",
    "# frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "# frameWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "# frameHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "# print(frameCount)\n",
    "# # video = np.empty((frameCount, frameHeight, frameWidth, 3), np.dtype('uint8'))\n",
    "\n",
    "# #         fc = 0\n",
    "# #         ret = True\n",
    "\n",
    "# #         while (fc < frameCount  and ret):\n",
    "# #             ret, video[fc] = cap.read()\n",
    "# #             fc += 1\n",
    "\n",
    "# #         cap.release()\n",
    "# #         return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_id=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = FileSourceDataset(TextDataSource(data_root, speaker_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mel = FileSourceDataset(MelSpecDataSource(data_root, speaker_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = FileSourceDataset(LinearSpecDataSource(data_root, speaker_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = FileSourceDataset(VisualDataSource(data_root, speaker_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_lengths = Mel.file_data_source.frame_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = PartialyRandomizedSimilarTimeLengthSampler(\n",
    "         frame_lengths, batch_size=hparams.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PyTorchDataset(X, Mel, Y, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = data_utils.DataLoader(\n",
    "         dataset, batch_size=hparams.batch_size,\n",
    "         num_workers=hparams.num_workers, sampler=sampler,\n",
    "         collate_fn=collate_fn, pin_memory=hparams.pin_memory, drop_last=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # d = np.array([x[3] for x in dataset[0]],dtype=np.float32)\n",
    "# #     v_batch = torch.FloatTensor(d)\n",
    "# max=0\n",
    "# for x in dataset:\n",
    "#     print(x[3].shape)\n",
    "#     a=x[3].shape\n",
    "#     print(a[0])\n",
    "#     if max < a[0]:\n",
    "#         max = a[0]\n",
    "# #     d = x[3]\n",
    "    \n",
    "# print(max)\n",
    "#     # text positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V.collected_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "torch.Size([4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for step, (x, lengths, mel, y, v, positions, done, target_lengths,\n",
    "                    speaker_ids) in tqdm(enumerate(data_loader)):\n",
    "    a,b=lengths\n",
    "    print(a.shape)\n",
    "    print(b.shape)\n",
    "    break;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = getattr(builder, hparams.builder)(\n",
    "        n_speakers=hparams.n_speakers,\n",
    "        speaker_embed_dim=hparams.speaker_embed_dim,\n",
    "        n_vocab=_frontend.n_vocab,\n",
    "        embed_dim=hparams.text_embed_dim,\n",
    "        mel_dim=hparams.num_mels,\n",
    "        linear_dim=hparams.fft_size // 2 + 1,\n",
    "        r=hparams.outputs_per_step,\n",
    "        downsample_step=hparams.downsample_step,\n",
    "        padding_idx=hparams.padding_idx,\n",
    "        dropout=hparams.dropout,\n",
    "        kernel_size=hparams.kernel_size,\n",
    "        encoder_channels=hparams.encoder_channels,\n",
    "        decoder_channels=hparams.decoder_channels,\n",
    "        converter_channels=hparams.converter_channels,\n",
    "        use_memory_mask=hparams.use_memory_mask,\n",
    "        trainable_positional_encodings=hparams.trainable_positional_encodings,\n",
    "        force_monotonic_attention=hparams.force_monotonic_attention,\n",
    "        use_decoder_state_for_postnet_input=hparams.use_decoder_state_for_postnet_input,\n",
    "        max_positions=hparams.max_positions,\n",
    "        speaker_embedding_weight_std=hparams.speaker_embedding_weight_std,\n",
    "        freeze_embedding=hparams.freeze_embedding,\n",
    "        window_ahead=hparams.window_ahead,\n",
    "        window_backward=hparams.window_backward,\n",
    "        key_projection=hparams.key_projection,\n",
    "        value_projection=hparams.value_projection,\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "# print(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    " optimizer = optim.Adam(model.get_trainable_parameters(),\n",
    "                           lr=hparams.initial_learning_rate, betas=(\n",
    "        hparams.adam_beta1, hparams.adam_beta2),\n",
    "        eps=hparams.adam_eps, weight_decay=hparams.weight_decay,\n",
    "        amsgrad=hparams.amsgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_event_path = './'\n",
    "writer = SummaryWriter(log_event_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_dim = model.linear_dim\n",
    "r = hparams.outputs_per_step\n",
    "downsample_step = hparams.downsample_step\n",
    "current_lr = 0.002\n",
    "checkpoint_interval=None\n",
    "checkpoint_dir=None\n",
    "nepochs=None\n",
    "clip_thresh=1.0\n",
    "train_seq2seq=True\n",
    "train_postnet=True\n",
    "binary_criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec_loss(y_hat, y, mask, priority_bin=None, priority_w=0):\n",
    "    masked_l1 = MaskedL1Loss()\n",
    "    l1 = nn.L1Loss()\n",
    "\n",
    "    w = hparams.masked_loss_weight\n",
    "\n",
    "    # L1 loss\n",
    "    if w > 0:\n",
    "        assert mask is not None\n",
    "        l1_loss = w * masked_l1(y_hat, y, mask=mask) + (1 - w) * l1(y_hat, y)\n",
    "    else:\n",
    "        assert mask is None\n",
    "        l1_loss = l1(y_hat, y)\n",
    "\n",
    "    # Priority L1 loss\n",
    "    if priority_bin is not None and priority_w > 0:\n",
    "        if w > 0:\n",
    "            priority_loss = w * masked_l1(\n",
    "                y_hat[:, :, :priority_bin], y[:, :, :priority_bin], mask=mask) \\\n",
    "                + (1 - w) * l1(y_hat[:, :, :priority_bin], y[:, :, :priority_bin])\n",
    "        else:\n",
    "            priority_loss = l1(y_hat[:, :, :priority_bin], y[:, :, :priority_bin])\n",
    "        l1_loss = (1 - priority_w) * l1_loss + priority_w * priority_loss\n",
    "\n",
    "    # Binary divergence loss\n",
    "    if hparams.binary_divergence_weight <= 0:\n",
    "        binary_div = y.data.new(1).zero_()\n",
    "    else:\n",
    "        y_hat_logits = logit(y_hat)\n",
    "        z = -y * y_hat_logits + torch.log1p(torch.exp(y_hat_logits))\n",
    "        if w > 0:\n",
    "            binary_div = w * masked_mean(z, mask) + (1 - w) * z.mean()\n",
    "        else:\n",
    "            binary_div = z.mean()\n",
    "\n",
    "    return l1_loss, binary_div\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, model, data_loader, optimizer, writer,\n",
    "          init_lr=0.002,\n",
    "          checkpoint_dir=None, checkpoint_interval=None, nepochs=None,\n",
    "          clip_thresh=1.0,\n",
    "          train_seq2seq=True, train_postnet=True):\n",
    "    linear_dim = model.linear_dim\n",
    "    r = hparams.outputs_per_step\n",
    "    downsample_step = hparams.downsample_step\n",
    "    current_lr = init_lr\n",
    "    print(device)\n",
    "    binary_criterion = nn.BCELoss()\n",
    "\n",
    "    assert train_seq2seq or train_postnet\n",
    "\n",
    "    global global_step, global_epoch\n",
    "    while global_epoch < nepochs:\n",
    "        running_loss = 0.\n",
    "        for step, (x, input_audvid_lengths, mel, y, v, positions, done, target_lengths,\n",
    "                   speaker_ids) \\\n",
    "                in tqdm(enumerate(data_loader)):\n",
    "            model.train()\n",
    "            ismultispeaker = speaker_ids is not None\n",
    "            # Learning rate schedule\n",
    "            if hparams.lr_schedule is not None:\n",
    "                lr_schedule_f = getattr(lrschedule, hparams.lr_schedule)\n",
    "                current_lr = lr_schedule_f(\n",
    "                    init_lr, global_step, **hparams.lr_schedule_kwargs)\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = current_lr\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Used for Position encoding\n",
    "            text_positions, frame_positions = positions\n",
    "            input_lengths, video_length = input_audvid_lengths \n",
    "            # Downsample mel spectrogram\n",
    "            if downsample_step > 1:\n",
    "                mel = mel[:, 0::downsample_step, :].contiguous()\n",
    "\n",
    "            # Lengths\n",
    "            input_lengths = input_lengths.long().numpy()\n",
    "            decoder_lengths = target_lengths.long().numpy() // r // downsample_step\n",
    "\n",
    "            max_seq_len = max(input_lengths.max(), decoder_lengths.max())\n",
    "            if max_seq_len >= hparams.max_positions:\n",
    "                raise RuntimeError(\n",
    "                    \"\"\"max_seq_len ({}) >= max_posision ({})\n",
    "Input text or decoder targget length exceeded the maximum length.\n",
    "Please set a larger value for ``max_position`` in hyper parameters.\"\"\".format(\n",
    "                        max_seq_len, hparams.max_positions))\n",
    "\n",
    "            # Transform data to CUDA device\n",
    "            if train_seq2seq:\n",
    "                x = x.to(device)\n",
    "                text_positions = text_positions.to(device)\n",
    "                frame_positions = frame_positions.to(device)\n",
    "            if train_postnet:\n",
    "                y = y.to(device)\n",
    "            video_length = video_length.to(device)\n",
    "            v = v.to(device)\n",
    "            print(video_length)\n",
    "            mel, done = mel.to(device), done.to(device)\n",
    "            target_lengths = target_lengths.to(device)\n",
    "            speaker_ids = speaker_ids.to(device) if ismultispeaker else None\n",
    "\n",
    "            # Create mask if we use masked loss\n",
    "            if hparams.masked_loss_weight > 0:\n",
    "                # decoder output domain mask\n",
    "                decoder_target_mask = sequence_mask(\n",
    "                    target_lengths / (r * downsample_step),\n",
    "                    max_len=mel.size(1)).unsqueeze(-1)\n",
    "                if downsample_step > 1:\n",
    "                    # spectrogram-domain mask\n",
    "                    target_mask = sequence_mask(\n",
    "                        target_lengths, max_len=y.size(1)).unsqueeze(-1)\n",
    "                else:\n",
    "                    target_mask = decoder_target_mask\n",
    "                # shift mask\n",
    "                decoder_target_mask = decoder_target_mask[:, r:, :]\n",
    "                target_mask = target_mask[:, r:, :]\n",
    "            else:\n",
    "                decoder_target_mask, target_mask = None, None\n",
    "\n",
    "            # Apply model\n",
    "            if train_seq2seq and train_postnet:\n",
    "#                 print('l')\n",
    "                mel_outputs, linear_outputs, attn, done_hat, attn_f = model(\n",
    "                    x, mel, speaker_ids=speaker_ids,\n",
    "                    text_positions=text_positions, frame_positions=frame_positions,\n",
    "                    input_lengths=input_lengths, faces=v, video_lengths=video_length)\n",
    "            elif train_seq2seq:\n",
    "                assert speaker_ids is None\n",
    "                mel_outputs, attn, done_hat, _ = model.seq2seq(\n",
    "                    x, mel,\n",
    "                    text_positions=text_positions, frame_positions=frame_positions,\n",
    "                    input_lengths=input_lengths)\n",
    "                # reshape\n",
    "                mel_outputs = mel_outputs.view(len(mel), -1, mel.size(-1))\n",
    "                linear_outputs = None\n",
    "            elif train_postnet:\n",
    "                assert speaker_ids is None\n",
    "                linear_outputs = model.postnet(mel)\n",
    "                mel_outputs, attn, done_hat = None, None, None\n",
    "\n",
    "            # Losses\n",
    "            w = hparams.binary_divergence_weight\n",
    "\n",
    "            # mel:\n",
    "            if train_seq2seq:\n",
    "                mel_l1_loss, mel_binary_div = spec_loss(\n",
    "                    mel_outputs[:, :-r, :], mel[:, r:, :], decoder_target_mask)\n",
    "                mel_loss = (1 - w) * mel_l1_loss + w * mel_binary_div\n",
    "\n",
    "            # done:\n",
    "            if train_seq2seq:\n",
    "                done_loss = binary_criterion(done_hat, done)\n",
    "\n",
    "            # linear:\n",
    "            if train_postnet:\n",
    "                n_priority_freq = int(hparams.priority_freq / (hparams.sample_rate * 0.5) * linear_dim)\n",
    "                linear_l1_loss, linear_binary_div = spec_loss(\n",
    "                    linear_outputs[:, :-r, :], y[:, r:, :], target_mask,\n",
    "                    priority_bin=n_priority_freq,\n",
    "                    priority_w=hparams.priority_freq_weight)\n",
    "                linear_loss = (1 - w) * linear_l1_loss + w * linear_binary_div\n",
    "\n",
    "            # Combine losses\n",
    "            if train_seq2seq and train_postnet:\n",
    "                loss = mel_loss + linear_loss + done_loss\n",
    "            elif train_seq2seq:\n",
    "                loss = mel_loss + done_loss\n",
    "            elif train_postnet:\n",
    "                loss = linear_loss\n",
    "\n",
    "            # attention\n",
    "            if train_seq2seq and hparams.use_guided_attention:\n",
    "                soft_mask = guided_attentions(input_lengths, decoder_lengths,\n",
    "                                              attn.size(-2),\n",
    "                                              g=hparams.guided_attention_sigma)\n",
    "                soft_mask = torch.from_numpy(soft_mask).to(device)\n",
    "                attn_loss = (attn * soft_mask).mean()\n",
    "                loss += attn_loss\n",
    "\n",
    "            if global_step > 0 and global_step % checkpoint_interval == 0:\n",
    "                save_states(\n",
    "                    global_step, writer, mel_outputs, linear_outputs, attn,\n",
    "                    mel, y, input_lengths, checkpoint_dir)\n",
    "                save_checkpoint(\n",
    "                    model, optimizer, global_step, checkpoint_dir, global_epoch,\n",
    "                    train_seq2seq, train_postnet)\n",
    "\n",
    "            if global_step > 0 and global_step % hparams.eval_interval == 0:\n",
    "                eval_model(global_step, writer, device, model,\n",
    "                           checkpoint_dir, ismultispeaker)\n",
    "\n",
    "            # Update\n",
    "            loss.backward()\n",
    "            if clip_thresh > 0:\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    model.get_trainable_parameters(), clip_thresh)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Logs\n",
    "            writer.add_scalar(\"loss\", float(loss.item()), global_step)\n",
    "            if train_seq2seq:\n",
    "                writer.add_scalar(\"done_loss\", float(done_loss.item()), global_step)\n",
    "                writer.add_scalar(\"mel loss\", float(mel_loss.item()), global_step)\n",
    "                writer.add_scalar(\"mel_l1_loss\", float(mel_l1_loss.item()), global_step)\n",
    "                writer.add_scalar(\"mel_binary_div_loss\", float(mel_binary_div.item()), global_step)\n",
    "            if train_postnet:\n",
    "                writer.add_scalar(\"linear_loss\", float(linear_loss.item()), global_step)\n",
    "                writer.add_scalar(\"linear_l1_loss\", float(linear_l1_loss.item()), global_step)\n",
    "                writer.add_scalar(\"linear_binary_div_loss\", float(linear_binary_div.item()), global_step)\n",
    "            if train_seq2seq and hparams.use_guided_attention:\n",
    "                writer.add_scalar(\"attn_loss\", float(attn_loss.item()), global_step)\n",
    "            if clip_thresh > 0:\n",
    "                writer.add_scalar(\"gradient norm\", grad_norm, global_step)\n",
    "            writer.add_scalar(\"learning rate\", current_lr, global_step)\n",
    "\n",
    "            global_step += 1\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        averaged_loss = running_loss / (len(data_loader))\n",
    "        writer.add_scalar(\"loss (per epoch)\", averaged_loss, global_epoch)\n",
    "        print(\"Loss: {}\".format(running_loss / (len(data_loader))))\n",
    "\n",
    "        global_epoch += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@jit(nopython=True)\n",
    "def guided_attention(N, max_N, T, max_T, g):\n",
    "    W = np.zeros((max_N, max_T), dtype=np.float32)\n",
    "    for n in range(N):\n",
    "        for t in range(T):\n",
    "            W[n, t] = 1 - np.exp(-(n / N - t / T)**2 / (2 * g * g))\n",
    "    return W\n",
    "\n",
    "\n",
    "def guided_attentions(input_lengths, target_lengths, max_target_len, g=0.2):\n",
    "    B = len(input_lengths)\n",
    "    max_input_len = input_lengths.max()\n",
    "    W = np.zeros((B, max_target_len, max_input_len), dtype=np.float32)\n",
    "    for b in range(B):\n",
    "        W[b] = guided_attention(input_lengths[b], max_input_len,\n",
    "                                target_lengths[b], max_target_len, g).T\n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([124, 125, 125, 125], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-89d5f83a814b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mnepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mclip_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_thresh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m           train_seq2seq=train_seq2seq, train_postnet=train_postnet)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-acb9c885ec02>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(device, model, data_loader, optimizer, writer, init_lr, checkpoint_dir, checkpoint_interval, nepochs, clip_thresh, train_seq2seq, train_postnet)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeaker_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspeaker_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mtext_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframe_positions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     input_lengths=input_lengths, faces=v, video_lengths=video_length)\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtrain_seq2seq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mspeaker_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deepvoice3_pytorch/deepvoice3_pytorch/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text_sequences, mel_targets, speaker_ids, text_positions, frame_positions, vid_positions, input_lengths, faces, video_lengths, batch_size)\u001b[0m\n\u001b[1;32m     77\u001b[0m         mel_outputs, alignments, done, decoder_states, alignments_f = self.seq2seq(\n\u001b[1;32m     78\u001b[0m             \u001b[0mtext_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeaker_embed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             text_positions, frame_positions, vid_positions, input_lengths, video_lengths, faces, batch_size)\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# Reshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deepvoice3_pytorch/deepvoice3_pytorch/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text_sequences, mel_targets, speaker_embed, text_positions, frame_positions, vid_positions, input_lengths, video_lengths, faces, batch_size)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_encoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mtext_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframe_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvid_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvid_positions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             speaker_embed=speaker_embed, lengths=input_lengths, video_lengths=video_lengths)\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;31m# print(alignments.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# print(alignments_f.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deepvoice3_pytorch/deepvoice3_pytorch/nyanko.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_out, face_encoder_out, inputs, text_positions, frame_positions, vid_positions, speaker_embed, lengths, video_lengths)\u001b[0m\n\u001b[1;32m    371\u001b[0m                         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtext_pos_embed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                         \u001b[0;31m## VIDEO ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                         \u001b[0mvid_pos_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_keys_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid_positions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m                         \u001b[0mkeys_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeys_f\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvid_pos_embed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mframe_positions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m         return F.embedding(\n\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1814\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "    train_seq2seq=True\n",
    "    train_postnet=True\n",
    "    train(device, model, data_loader, optimizer, writer,\n",
    "              init_lr=hparams.initial_learning_rate,\n",
    "              checkpoint_dir=checkpoint_dir,\n",
    "              checkpoint_interval=hparams.checkpoint_interval,\n",
    "              nepochs=hparams.nepochs,\n",
    "              clip_thresh=hparams.clip_thresh,\n",
    "              train_seq2seq=train_seq2seq, train_postnet=train_postnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_alignment(alignment, path, info=None):\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(\n",
    "        alignment,\n",
    "        aspect='auto',\n",
    "        origin='lower',\n",
    "        interpolation='none')\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    xlabel = 'Decoder timestep'\n",
    "    if info is not None:\n",
    "        xlabel += '\\n\\n' + info\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Encoder timestep')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, format='png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = join(data_root, \"train.txt\")\n",
    "with open(meta, \"rb\") as f:\n",
    "    lines = f.readlines()\n",
    "l = lines[0].decode(\"utf-8\").split(\"|\")\n",
    "print(l, len(l))\n",
    "texts = list(map(lambda l: l.decode(\"utf-8\").split(\"|\")[3], lines))\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_positions = np.array([_pad(np.arange(1, x + 1), 10)\n",
    "                               for x in [3,6,5,4]], dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 1\n",
    "downsample_step = 4\n",
    "done = np.array([_pad(np.zeros(x // r // downsample_step - 1),\n",
    "                      50, constant_values=1)\n",
    "                 for x in [40, 15, 33, 25, 20]])\n",
    "done = torch.FloatTensor(done)\n",
    "print(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Create batch\"\"\"\n",
    "    r = hparams.outputs_per_step\n",
    "    downsample_step = hparams.downsample_step\n",
    "    multi_speaker = len(batch[0]) == 4\n",
    "\n",
    "    # Lengths\n",
    "    input_lengths = [len(x[0]) for x in batch]\n",
    "    # length of texts\n",
    "    max_input_len = max(input_lengths)\n",
    "\n",
    "    target_lengths = [len(x[1]) for x in batch]\n",
    "    # length of mel spectrogram\n",
    "    \n",
    "    max_target_len = max(target_lengths)\n",
    "    if max_target_len % r != 0:\n",
    "        max_target_len += r - max_target_len % r\n",
    "        assert max_target_len % r == 0\n",
    "    if max_target_len % downsample_step != 0:\n",
    "        max_target_len += downsample_step - max_target_len % downsample_step\n",
    "        assert max_target_len % downsample_step == 0\n",
    "\n",
    "    # Set 0 for zero beginning padding\n",
    "    # imitates initial decoder states\n",
    "    b_pad = r\n",
    "    max_target_len += b_pad * downsample_step\n",
    "\n",
    "    a = np.array([_pad(x[0], max_input_len) for x in batch], dtype=np.int)\n",
    "    # padding text sequence to make it equal to max_input_len\n",
    "    x_batch = torch.LongTensor(a)\n",
    "\n",
    "    input_lengths = torch.LongTensor(input_lengths)\n",
    "    target_lengths = torch.LongTensor(target_lengths)\n",
    "\n",
    "    b = np.array([_pad_2d(x[1], max_target_len, b_pad=b_pad) for x in batch],\n",
    "                 dtype=np.float32)\n",
    "    mel_batch = torch.FloatTensor(b)\n",
    "    # similarly padding mel and y to max_target_len\n",
    "    c = np.array([_pad_2d(x[2], max_target_len, b_pad=b_pad) for x in batch],\n",
    "                 dtype=np.float32)\n",
    "    y_batch = torch.FloatTensor(c)\n",
    "\n",
    "    # text positions\n",
    "    text_positions = np.array([_pad(np.arange(1, len(x[0]) + 1), max_input_len)\n",
    "                               for x in batch], dtype=np.int)\n",
    "    \"\"\"\n",
    "    2d array having number from 1 to n for each character till max_input_len\n",
    "    text_positions = [[1, 2, 3, 4, 0, 0, 0, 0, 0, 0, 0],\n",
    "                      [1, 2, 3, 4, 5, 6, 7, 0, 0, 0, 0],\n",
    "                      [1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 0],\n",
    "                      [1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "    \"\"\"\n",
    "    text_positions = torch.LongTensor(text_positions)\n",
    "\n",
    "    max_decoder_target_len = max_target_len // r // downsample_step\n",
    "\n",
    "    # frame positions\n",
    "    s, e = 1, max_decoder_target_len + 1\n",
    "    # if b_pad > 0:\n",
    "    #    s, e = s - 1, e - 1\n",
    "    # NOTE: needs clone to supress RuntimeError in dataloarder...\n",
    "    # ref: https://github.com/pytorch/pytorch/issues/10756\n",
    "    frame_positions = torch.arange(s, e).long().unsqueeze(0).expand(\n",
    "        len(batch), max_decoder_target_len).clone()\n",
    "    \"\"\"\n",
    "    2d array having number from 1 to n for each character till max_decoder_target_len\n",
    "    frame_positions = [[1, 2, 3, 4, 5, 6, .... , max_decoder_target_len],\n",
    "                       [1, 2, 3, 4, 5, 6, .... , max_decoder_target_len],\n",
    "                       .\n",
    "                       .\n",
    "                       len(batch)]\n",
    "    \"\"\"\n",
    "    # done flags\n",
    "    done = np.array([_pad(np.zeros(len(x[1]) // r // downsample_step - 1),\n",
    "                          max_decoder_target_len, constant_values=1)\n",
    "                     for x in batch])\n",
    "    done = torch.FloatTensor(done).unsqueeze(-1)\n",
    "\n",
    "    if multi_speaker:\n",
    "        speaker_ids = torch.LongTensor([x[3] for x in batch])\n",
    "    else:\n",
    "        speaker_ids = None\n",
    "\n",
    "    return x_batch, input_lengths, mel_batch, y_batch, \\\n",
    "        (text_positions, frame_positions), done, target_lengths, speaker_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_frontend = getattr(frontend, hparams.frontend)\n",
    "model = getattr(builder, hparams.builder)(\n",
    "    n_speakers=hparams.n_speakers,\n",
    "    speaker_embed_dim=hparams.speaker_embed_dim,\n",
    "    n_vocab=_frontend.n_vocab,\n",
    "    embed_dim=hparams.text_embed_dim,\n",
    "    mel_dim=hparams.num_mels,\n",
    "    linear_dim=hparams.fft_size // 2 + 1,\n",
    "    r=hparams.outputs_per_step,\n",
    "    downsample_step=hparams.downsample_step,\n",
    "    padding_idx=hparams.padding_idx,\n",
    "    dropout=hparams.dropout,\n",
    "    kernel_size=hparams.kernel_size,\n",
    "    encoder_channels=hparams.encoder_channels,\n",
    "    decoder_channels=hparams.decoder_channels,\n",
    "    converter_channels=hparams.converter_channels,\n",
    "    use_memory_mask=hparams.use_memory_mask,\n",
    "    trainable_positional_encodings=hparams.trainable_positional_encodings,\n",
    "    force_monotonic_attention=hparams.force_monotonic_attention,\n",
    "    use_decoder_state_for_postnet_input=hparams.use_decoder_state_for_postnet_input,\n",
    "    max_positions=hparams.max_positions,\n",
    "    speaker_embedding_weight_std=hparams.speaker_embedding_weight_std,\n",
    "    freeze_embedding=hparams.freeze_embedding,\n",
    "    window_ahead=hparams.window_ahead,\n",
    "    window_backward=hparams.window_backward,\n",
    "    key_projection=hparams.key_projection,\n",
    "    value_projection=hparams.value_projection,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/scratch/faizan/trn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_id = None\n",
    "X = FileSourceDataset(TextDataSource(data_root, speaker_id))\n",
    "Mel = FileSourceDataset(MelSpecDataSource(data_root, speaker_id))\n",
    "Y = FileSourceDataset(LinearSpecDataSource(data_root, speaker_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sampler\n",
    "frame_lengths = Mel.file_data_source.frame_lengths\n",
    "sampler = PartialyRandomizedSimilarTimeLengthSampler(\n",
    "    frame_lengths, batch_size=hparams.batch_size)\n",
    "\n",
    "# Dataset and Dataloader setup\n",
    "dataset = PyTorchDataset(X, Mel, Y)\n",
    "data_loader = data_utils.DataLoader(\n",
    "    dataset, batch_size=hparams.batch_size,\n",
    "    num_workers=hparams.num_workers, sampler=sampler,\n",
    "    collate_fn=collate_fn, pin_memory=hparams.pin_memory, drop_last=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def position_encoding_init(n_position, d_pos_vec, position_rate=1.0,\n",
    "                           sinusoidal=True):\n",
    "    ''' Init the sinusoid position encoding table '''\n",
    "\n",
    "    # keep dim 0 for padding token position encoding zero vector\n",
    "    position_enc = np.array([\n",
    "        [position_rate * pos / np.power(10000, 2 * (i // 2) / d_pos_vec) for i in range(d_pos_vec)]\n",
    "        if pos != 0 else np.zeros(d_pos_vec) for pos in range(n_position)])\n",
    "\n",
    "    position_enc = torch.from_numpy(position_enc).float()\n",
    "    if sinusoidal:\n",
    "        position_enc[1:, 0::2] = torch.sin(position_enc[1:, 0::2])  # dim 2i\n",
    "        position_enc[1:, 1::2] = torch.cos(position_enc[1:, 1::2])  # dim 2i+1\n",
    "\n",
    "    return position_enc\n",
    "\n",
    "\n",
    "def sinusoidal_encode(x, w):\n",
    "    y = w * x\n",
    "    y[1:, 0::2] = torch.sin(y[1:, 0::2].clone())\n",
    "    y[1:, 1::2] = torch.cos(y[1:, 1::2].clone())\n",
    "    return y\n",
    "\n",
    "\n",
    "class SinusoidalEncoding(nn.Embedding):\n",
    "\n",
    "    def __init__(self, num_embeddings, embedding_dim,\n",
    "                 *args, **kwargs):\n",
    "        super(SinusoidalEncoding, self).__init__(num_embeddings, embedding_dim,\n",
    "                                                 padding_idx=0,\n",
    "                                                 *args, **kwargs)\n",
    "        self.weight.data = position_encoding_init(num_embeddings, embedding_dim,\n",
    "                                                  position_rate=1.0,\n",
    "                                                  sinusoidal=False)\n",
    "\n",
    "    def forward(self, x, w=1.0):\n",
    "        isscaler = np.isscalar(w)\n",
    "        assert self.padding_idx is not None\n",
    "\n",
    "        if isscaler or w.size(0) == 1:\n",
    "            weight = sinusoidal_encode(self.weight, w)\n",
    "            return F.embedding(\n",
    "                x, weight, self.padding_idx, self.max_norm,\n",
    "                self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
    "        else:\n",
    "            # TODO: cannot simply apply for batch\n",
    "            # better to implement efficient function\n",
    "            pe = []\n",
    "            for batch_idx, we in enumerate(w):\n",
    "                weight = sinusoidal_encode(self.weight, we)\n",
    "                pe.append(F.embedding(\n",
    "                    x[batch_idx], weight, self.padding_idx, self.max_norm,\n",
    "                    self.norm_type, self.scale_grad_by_freq, self.sparse))\n",
    "            pe = torch.stack(pe)\n",
    "            return pe\n",
    "\n",
    "\n",
    "class GradMultiply(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, scale):\n",
    "        ctx.scale = scale\n",
    "        res = x.new(x)\n",
    "        ctx.mark_shared_storage((x, res))\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad):\n",
    "        return grad * ctx.scale, None\n",
    "\n",
    "\n",
    "def Linear(in_features, out_features, dropout=0):\n",
    "    \"\"\"Weight-normalized Linear layer (input: N x T x C)\"\"\"\n",
    "    m = nn.Linear(in_features, out_features)\n",
    "    m.weight.data.normal_(mean=0, std=math.sqrt((1 - dropout) / in_features))\n",
    "    m.bias.data.zero_()\n",
    "    return nn.utils.weight_norm(m)\n",
    "\n",
    "\n",
    "def Embedding(num_embeddings, embedding_dim, padding_idx, std=0.01):\n",
    "    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
    "    m.weight.data.normal_(0, std)\n",
    "    return m\n",
    "\n",
    "\n",
    "def Conv1d(in_channels, out_channels, kernel_size, dropout=0, std_mul=4.0, **kwargs):\n",
    "    from .conv import Conv1d\n",
    "    m = Conv1d(in_channels, out_channels, kernel_size, **kwargs)\n",
    "    std = math.sqrt((std_mul * (1.0 - dropout)) / (m.kernel_size[0] * in_channels))\n",
    "    m.weight.data.normal_(mean=0, std=std)\n",
    "    m.bias.data.zero_()\n",
    "    return nn.utils.weight_norm(m)\n",
    "\n",
    "\n",
    "def ConvTranspose1d(in_channels, out_channels, kernel_size, dropout=0,\n",
    "                    std_mul=1.0, **kwargs):\n",
    "    m = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, **kwargs)\n",
    "    std = math.sqrt((std_mul * (1.0 - dropout)) / (m.kernel_size[0] * in_channels))\n",
    "    m.weight.data.normal_(mean=0, std=std)\n",
    "    m.bias.data.zero_()\n",
    "    return nn.utils.weight_norm(m)\n",
    "\n",
    "\n",
    "class Conv1dGLU(nn.Module):\n",
    "    \"\"\"(Dilated) Conv1d + Gated linear unit + (optionally) speaker embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_speakers, speaker_embed_dim,\n",
    "                 in_channels, out_channels, kernel_size,\n",
    "                 dropout, padding=None, dilation=1, causal=False, residual=False,\n",
    "                 *args, **kwargs):\n",
    "        super(Conv1dGLU, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.residual = residual\n",
    "        if padding is None:\n",
    "            # no future time stamps available\n",
    "            if causal:\n",
    "                padding = (kernel_size - 1) * dilation\n",
    "            else:\n",
    "                padding = (kernel_size - 1) // 2 * dilation\n",
    "        self.causal = causal\n",
    "\n",
    "        self.conv = Conv1d(in_channels, 2 * out_channels, kernel_size,\n",
    "                           dropout=dropout, padding=padding, dilation=dilation,\n",
    "                           *args, **kwargs)\n",
    "        if n_speakers > 1:\n",
    "            self.speaker_proj = Linear(speaker_embed_dim, out_channels)\n",
    "        else:\n",
    "            self.speaker_proj = None\n",
    "\n",
    "    def forward(self, x, speaker_embed=None):\n",
    "        return self._forward(x, speaker_embed, False)\n",
    "\n",
    "    def incremental_forward(self, x, speaker_embed=None):\n",
    "        return self._forward(x, speaker_embed, True)\n",
    "\n",
    "    def _forward(self, x, speaker_embed, is_incremental):\n",
    "        residual = x\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        if is_incremental:\n",
    "            splitdim = -1\n",
    "            x = self.conv.incremental_forward(x)\n",
    "        else:\n",
    "            splitdim = 1\n",
    "            x = self.conv(x)\n",
    "            # remove future time steps\n",
    "            x = x[:, :, :residual.size(-1)] if self.causal else x\n",
    "\n",
    "        a, b = x.split(x.size(splitdim) // 2, dim=splitdim)\n",
    "        if self.speaker_proj is not None:\n",
    "            softsign = F.softsign(self.speaker_proj(speaker_embed))\n",
    "            # Since conv layer assumes BCT, we need to transpose\n",
    "            softsign = softsign if is_incremental else softsign.transpose(1, 2)\n",
    "            a = a + softsign\n",
    "        x = a * torch.sigmoid(b)\n",
    "        return (x + residual) * math.sqrt(0.5) if self.residual else x\n",
    "\n",
    "    def clear_buffer(self):\n",
    "        self.conv.clear_buffer()\n",
    "\n",
    "\n",
    "class HighwayConv1d(nn.Module):\n",
    "    \"\"\"Weight normzlized Conv1d + Highway network (support incremental forward)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, padding=None,\n",
    "                 dilation=1, causal=False, dropout=0, std_mul=None, glu=False):\n",
    "        super(HighwayConv1d, self).__init__()\n",
    "        if std_mul is None:\n",
    "            std_mul = 4.0 if glu else 1.0\n",
    "        if padding is None:\n",
    "            # no future time stamps available\n",
    "            if causal:\n",
    "                padding = (kernel_size - 1) * dilation\n",
    "            else:\n",
    "                padding = (kernel_size - 1) // 2 * dilation\n",
    "        self.causal = causal\n",
    "        self.dropout = dropout\n",
    "        self.glu = glu\n",
    "\n",
    "        self.conv = Conv1d(in_channels, 2 * out_channels,\n",
    "                           kernel_size=kernel_size, padding=padding,\n",
    "                           dilation=dilation, dropout=dropout,\n",
    "                           std_mul=std_mul)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward(x, False)\n",
    "\n",
    "    def incremental_forward(self, x):\n",
    "        return self._forward(x, True)\n",
    "\n",
    "    def _forward(self, x, is_incremental):\n",
    "        \"\"\"Forward\n",
    "\n",
    "        Args:\n",
    "            x: (B, in_channels, T)\n",
    "        returns:\n",
    "            (B, out_channels, T)\n",
    "        \"\"\"\n",
    "\n",
    "        residual = x\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        if is_incremental:\n",
    "            splitdim = -1\n",
    "            x = self.conv.incremental_forward(x)\n",
    "        else:\n",
    "            splitdim = 1\n",
    "            x = self.conv(x)\n",
    "            # remove future time steps\n",
    "            x = x[:, :, :residual.size(-1)] if self.causal else x\n",
    "\n",
    "        if self.glu:\n",
    "            x = F.glu(x, dim=splitdim)\n",
    "            return (x + residual) * math.sqrt(0.5)\n",
    "        else:\n",
    "            a, b = x.split(x.size(splitdim) // 2, dim=splitdim)\n",
    "            T = torch.sigmoid(b)\n",
    "            return (T * a + (1 - T) * residual)\n",
    "\n",
    "    def clear_buffer(self):\n",
    "        self.conv.clear_buffer()\n",
    "\n",
    "\n",
    "def get_mask_from_lengths(memory, memory_lengths):\n",
    "    \"\"\"Get mask tensor from list of length\n",
    "    Args:\n",
    "        memory: (batch, max_time, dim)\n",
    "        memory_lengths: array like\n",
    "    \"\"\"\n",
    "    max_len = max(memory_lengths)\n",
    "    mask = torch.arange(max_len).expand(memory.size(0), max_len) < torch.tensor(memory_lengths).unsqueeze(-1)\n",
    "    mask = mask.to(memory.device)\n",
    "    return ~mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
    "m = nn.Embedding(15, 10, padding_idx=0)\n",
    "print(m)\n",
    "print(m(torch.tensor([10,2])))\n",
    "m.weight.data.normal_(0, 0.01)\n",
    "print(m)\n",
    "print(m(torch.tensor([13,9, 8])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_dim, n_speakers, speaker_embed_dim,\n",
    "                 padding_idx=None, embedding_weight_std=0.1,\n",
    "                 convolutions=((64, 5, .1),) * 7,\n",
    "                 max_positions=512, dropout=0.1, apply_grad_scaling=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.num_attention_layers = None\n",
    "        self.apply_grad_scaling = apply_grad_scaling\n",
    "\n",
    "        # Text input embeddings\n",
    "        self.embed_tokens = Embedding(\n",
    "            n_vocab, embed_dim, padding_idx, embedding_weight_std)\n",
    "\n",
    "        # Speaker embedding\n",
    "        if n_speakers > 1:\n",
    "            self.speaker_fc1 = Linear(speaker_embed_dim, embed_dim, dropout=dropout)\n",
    "            self.speaker_fc2 = Linear(speaker_embed_dim, embed_dim, dropout=dropout)\n",
    "        self.n_speakers = n_speakers\n",
    "\n",
    "        # Non causual convolution blocks\n",
    "        in_channels = embed_dim\n",
    "        self.convolutions = nn.ModuleList()\n",
    "        std_mul = 1.0\n",
    "        for (out_channels, kernel_size, dilation) in convolutions:\n",
    "            if in_channels != out_channels:\n",
    "                # Conv1d + ReLU\n",
    "                self.convolutions.append(\n",
    "                    Conv1d(in_channels, out_channels, kernel_size=1, padding=0,\n",
    "                           dilation=1, std_mul=std_mul))\n",
    "                self.convolutions.append(nn.ReLU(inplace=True))\n",
    "                in_channels = out_channels\n",
    "                std_mul = 2.0\n",
    "            self.convolutions.append(\n",
    "                Conv1dGLU(n_speakers, speaker_embed_dim,\n",
    "                          in_channels, out_channels, kernel_size, causal=False,\n",
    "                          dilation=dilation, dropout=dropout, std_mul=std_mul,\n",
    "                          residual=True))\n",
    "            in_channels = out_channels\n",
    "            std_mul = 4.0\n",
    "        # Last 1x1 convolution\n",
    "        self.convolutions.append(Conv1d(in_channels, embed_dim, kernel_size=1,\n",
    "                                        padding=0, dilation=1, std_mul=std_mul,\n",
    "                                        dropout=dropout))\n",
    "\n",
    "    def forward(self, text_sequences, text_positions=None, lengths=None,\n",
    "                speaker_embed=None):\n",
    "        assert self.n_speakers == 1 or speaker_embed is not None\n",
    "\n",
    "        # embed text_sequences\n",
    "        x = self.embed_tokens(text_sequences.long())\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # expand speaker embedding for all time steps\n",
    "        speaker_embed_btc = expand_speaker_embed(x, speaker_embed)\n",
    "        if speaker_embed_btc is not None:\n",
    "            speaker_embed_btc = F.dropout(speaker_embed_btc, p=self.dropout, training=self.training)\n",
    "            x = x + F.softsign(self.speaker_fc1(speaker_embed_btc))\n",
    "\n",
    "        input_embedding = x\n",
    "\n",
    "        # B x T x C -> B x C x T\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # D conv blocks\n",
    "        for f in self.convolutions:\n",
    "            x = f(x, speaker_embed_btc) if isinstance(f, Conv1dGLU) else f(x)\n",
    "\n",
    "        # Back to B x T x C\n",
    "        keys = x.transpose(1, 2)\n",
    "\n",
    "        if speaker_embed_btc is not None:\n",
    "            keys = keys + F.softsign(self.speaker_fc2(speaker_embed_btc))\n",
    "\n",
    "        # scale gradients (this only affects backward, not forward)\n",
    "        if self.apply_grad_scaling and self.num_attention_layers is not None:\n",
    "            keys = GradMultiply.apply(keys, 1.0 / (2.0 * self.num_attention_layers))\n",
    "\n",
    "        # add output to input embedding for attention\n",
    "        values = (keys + input_embedding) * math.sqrt(0.5)\n",
    "\n",
    "        return keys, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_dim, n_speakers, speaker_embed_dim,\n",
    "                 in_dim=80, r=5,\n",
    "                 max_positions=512, padding_idx=None,\n",
    "                 preattention=((128, 5, 1),) * 4,\n",
    "                 convolutions=((128, 5, 1),) * 4,\n",
    "                 attention=True, dropout=0.1,\n",
    "                 use_memory_mask=False,\n",
    "                 force_monotonic_attention=False,\n",
    "                 query_position_rate=1.0,\n",
    "                 key_position_rate=1.29,\n",
    "                 window_ahead=3,\n",
    "                 window_backward=1,\n",
    "                 key_projection=True,\n",
    "                 value_projection=True,\n",
    "                 ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_dim = in_dim\n",
    "        self.r = r\n",
    "        self.query_position_rate = query_position_rate\n",
    "        self.key_position_rate = key_position_rate\n",
    "\n",
    "        in_channels = in_dim * r\n",
    "        if isinstance(attention, bool):\n",
    "            # expand True into [True, True, ...] and do the same with False\n",
    "            attention = [attention] * len(convolutions)\n",
    "\n",
    "        # Position encodings for query (decoder states) and keys (encoder states)\n",
    "        self.embed_query_positions = SinusoidalEncoding(\n",
    "            max_positions, convolutions[0][0])\n",
    "        self.embed_keys_positions = SinusoidalEncoding(\n",
    "            max_positions, embed_dim)\n",
    "        # Used for compute multiplier for positional encodings\n",
    "        if n_speakers > 1:\n",
    "            self.speaker_proj1 = Linear(speaker_embed_dim, 1, dropout=dropout)\n",
    "            self.speaker_proj2 = Linear(speaker_embed_dim, 1, dropout=dropout)\n",
    "        else:\n",
    "            self.speaker_proj1, self.speaker_proj2 = None, None\n",
    "\n",
    "        # Prenet: causal convolution blocks\n",
    "        self.preattention = nn.ModuleList()\n",
    "        in_channels = in_dim * r\n",
    "        std_mul = 1.0\n",
    "        for out_channels, kernel_size, dilation in preattention:\n",
    "            if in_channels != out_channels:\n",
    "                # Conv1d + ReLU\n",
    "                self.preattention.append(\n",
    "                    Conv1d(in_channels, out_channels, kernel_size=1, padding=0,\n",
    "                           dilation=1, std_mul=std_mul))\n",
    "                self.preattention.append(nn.ReLU(inplace=True))\n",
    "                in_channels = out_channels\n",
    "                std_mul = 2.0\n",
    "            self.preattention.append(\n",
    "                Conv1dGLU(n_speakers, speaker_embed_dim,\n",
    "                          in_channels, out_channels, kernel_size, causal=True,\n",
    "                          dilation=dilation, dropout=dropout, std_mul=std_mul,\n",
    "                          residual=True))\n",
    "            in_channels = out_channels\n",
    "            std_mul = 4.0\n",
    "\n",
    "        # Causal convolution blocks + attention layers\n",
    "        self.convolutions = nn.ModuleList()\n",
    "        self.attention = nn.ModuleList()\n",
    "\n",
    "        for i, (out_channels, kernel_size, dilation) in enumerate(convolutions):\n",
    "            assert in_channels == out_channels\n",
    "            self.convolutions.append(\n",
    "                Conv1dGLU(n_speakers, speaker_embed_dim,\n",
    "                          in_channels, out_channels, kernel_size, causal=True,\n",
    "                          dilation=dilation, dropout=dropout, std_mul=std_mul,\n",
    "                          residual=False))\n",
    "            self.attention.append(\n",
    "                AttentionLayer(out_channels, embed_dim,\n",
    "                               dropout=dropout,\n",
    "                               window_ahead=window_ahead,\n",
    "                               window_backward=window_backward,\n",
    "                               key_projection=key_projection,\n",
    "                               value_projection=value_projection)\n",
    "                if attention[i] else None)\n",
    "            in_channels = out_channels\n",
    "            std_mul = 4.0\n",
    "        # Last 1x1 convolution\n",
    "        self.last_conv = Conv1d(in_channels, in_dim * r, kernel_size=1,\n",
    "                                padding=0, dilation=1, std_mul=std_mul,\n",
    "                                dropout=dropout)\n",
    "\n",
    "        # Mel-spectrogram (before sigmoid) -> Done binary flag\n",
    "        self.fc = Linear(in_dim * r, 1)\n",
    "\n",
    "        self.max_decoder_steps = 200\n",
    "        self.min_decoder_steps = 10\n",
    "        self.use_memory_mask = use_memory_mask\n",
    "        if isinstance(force_monotonic_attention, bool):\n",
    "            self.force_monotonic_attention = [force_monotonic_attention] * len(convolutions)\n",
    "        else:\n",
    "            self.force_monotonic_attention = force_monotonic_attention\n",
    "\n",
    "    def forward(self, encoder_out, inputs=None,\n",
    "                text_positions=None, frame_positions=None,\n",
    "                speaker_embed=None, lengths=None):\n",
    "        if inputs is None:\n",
    "            assert text_positions is not None\n",
    "            self.start_fresh_sequence()\n",
    "            outputs = self.incremental_forward(encoder_out, text_positions, speaker_embed)\n",
    "            return outputs\n",
    "\n",
    "        # Grouping multiple frames if necessary\n",
    "        if inputs.size(-1) == self.in_dim:\n",
    "            inputs = inputs.view(inputs.size(0), inputs.size(1) // self.r, -1)\n",
    "        assert inputs.size(-1) == self.in_dim * self.r\n",
    "\n",
    "        # expand speaker embedding for all time steps\n",
    "        speaker_embed_btc = expand_speaker_embed(inputs, speaker_embed)\n",
    "        if speaker_embed_btc is not None:\n",
    "            speaker_embed_btc = F.dropout(speaker_embed_btc, p=self.dropout, training=self.training)\n",
    "\n",
    "        keys, values = encoder_out\n",
    "\n",
    "        if self.use_memory_mask and lengths is not None:\n",
    "            mask = get_mask_from_lengths(keys, lengths)\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        # position encodings\n",
    "        if text_positions is not None:\n",
    "            w = self.key_position_rate\n",
    "            # TODO: may be useful to have projection per attention layer\n",
    "            if self.speaker_proj1 is not None:\n",
    "                w = w * torch.sigmoid(self.speaker_proj1(speaker_embed)).view(-1)\n",
    "            text_pos_embed = self.embed_keys_positions(text_positions, w)\n",
    "            keys = keys + text_pos_embed\n",
    "        if frame_positions is not None:\n",
    "            w = self.query_position_rate\n",
    "            if self.speaker_proj2 is not None:\n",
    "                w = w * torch.sigmoid(self.speaker_proj2(speaker_embed)).view(-1)\n",
    "            frame_pos_embed = self.embed_query_positions(frame_positions, w)\n",
    "\n",
    "        # transpose only once to speed up attention layers\n",
    "        keys = keys.transpose(1, 2).contiguous()\n",
    "\n",
    "        x = inputs\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Generic case: B x T x C -> B x C x T\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # Prenet\n",
    "        for f in self.preattention:\n",
    "            x = f(x, speaker_embed_btc) if isinstance(f, Conv1dGLU) else f(x)\n",
    "\n",
    "        # Casual convolutions + Multi-hop attentions\n",
    "        alignments = []\n",
    "        for f, attention in zip(self.convolutions, self.attention):\n",
    "            residual = x\n",
    "\n",
    "            x = f(x, speaker_embed_btc) if isinstance(f, Conv1dGLU) else f(x)\n",
    "\n",
    "            # Feed conv output to attention layer as query\n",
    "            if attention is not None:\n",
    "                assert isinstance(f, Conv1dGLU)\n",
    "                # (B x T x C)\n",
    "                x = x.transpose(1, 2)\n",
    "                x = x if frame_positions is None else x + frame_pos_embed\n",
    "                x, alignment = attention(x, (keys, values), mask=mask)\n",
    "                # (T x B x C)\n",
    "                x = x.transpose(1, 2)\n",
    "                alignments += [alignment]\n",
    "\n",
    "            if isinstance(f, Conv1dGLU):\n",
    "                x = (x + residual) * math.sqrt(0.5)\n",
    "\n",
    "        # decoder state (B x T x C):\n",
    "        # internal representation before compressed to output dimention\n",
    "        decoder_states = x.transpose(1, 2).contiguous()\n",
    "        x = self.last_conv(x)\n",
    "\n",
    "        # Back to B x T x C\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # project to mel-spectorgram\n",
    "        outputs = torch.sigmoid(x)\n",
    "\n",
    "        # Done flag\n",
    "        done = torch.sigmoid(self.fc(x))\n",
    "\n",
    "        return outputs, torch.stack(alignments), done, decoder_states\n",
    "\n",
    "    def incremental_forward(self, encoder_out, text_positions, speaker_embed=None,\n",
    "                            initial_input=None, test_inputs=None):\n",
    "        keys, values = encoder_out\n",
    "        B = keys.size(0)\n",
    "\n",
    "        # position encodings\n",
    "        w = self.key_position_rate\n",
    "        # TODO: may be useful to have projection per attention layer\n",
    "        if self.speaker_proj1 is not None:\n",
    "            w = w * torch.sigmoid(self.speaker_proj1(speaker_embed)).view(-1)\n",
    "        text_pos_embed = self.embed_keys_positions(text_positions, w)\n",
    "        keys = keys + text_pos_embed\n",
    "\n",
    "        # transpose only once to speed up attention layers\n",
    "        keys = keys.transpose(1, 2).contiguous()\n",
    "\n",
    "        decoder_states = []\n",
    "        outputs = []\n",
    "        alignments = []\n",
    "        dones = []\n",
    "        # intially set to zeros\n",
    "        last_attended = [None] * len(self.attention)\n",
    "        for idx, v in enumerate(self.force_monotonic_attention):\n",
    "            last_attended[idx] = 0 if v else None\n",
    "\n",
    "        num_attention_layers = sum([layer is not None for layer in self.attention])\n",
    "        t = 0\n",
    "        if initial_input is None:\n",
    "            initial_input = keys.data.new(B, 1, self.in_dim * self.r).zero_()\n",
    "        current_input = initial_input\n",
    "        while True:\n",
    "            # frame pos start with 1.\n",
    "            frame_pos = keys.data.new(B, 1).fill_(t + 1).long()\n",
    "            w = self.query_position_rate\n",
    "            if self.speaker_proj2 is not None:\n",
    "                w = w * torch.sigmoid(self.speaker_proj2(speaker_embed)).view(-1)\n",
    "            frame_pos_embed = self.embed_query_positions(frame_pos, w)\n",
    "\n",
    "            if test_inputs is not None:\n",
    "                if t >= test_inputs.size(1):\n",
    "                    break\n",
    "                current_input = test_inputs[:, t, :].unsqueeze(1)\n",
    "            else:\n",
    "                if t > 0:\n",
    "                    current_input = outputs[-1]\n",
    "            x = current_input\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "            # Prenet\n",
    "            for f in self.preattention:\n",
    "                if isinstance(f, Conv1dGLU):\n",
    "                    x = f.incremental_forward(x, speaker_embed)\n",
    "                else:\n",
    "                    try:\n",
    "                        x = f.incremental_forward(x)\n",
    "                    except AttributeError as e:\n",
    "                        x = f(x)\n",
    "\n",
    "            # Casual convolutions + Multi-hop attentions\n",
    "            ave_alignment = None\n",
    "            for idx, (f, attention) in enumerate(zip(self.convolutions,\n",
    "                                                     self.attention)):\n",
    "                residual = x\n",
    "                if isinstance(f, Conv1dGLU):\n",
    "                    x = f.incremental_forward(x, speaker_embed)\n",
    "                else:\n",
    "                    try:\n",
    "                        x = f.incremental_forward(x)\n",
    "                    except AttributeError as e:\n",
    "                        x = f(x)\n",
    "\n",
    "                # attention\n",
    "                if attention is not None:\n",
    "                    assert isinstance(f, Conv1dGLU)\n",
    "                    x = x + frame_pos_embed\n",
    "                    x, alignment = attention(x, (keys, values),\n",
    "                                             last_attended=last_attended[idx])\n",
    "                    if self.force_monotonic_attention[idx]:\n",
    "                        last_attended[idx] = alignment.max(-1)[1].view(-1).data[0]\n",
    "                    if ave_alignment is None:\n",
    "                        ave_alignment = alignment\n",
    "                    else:\n",
    "                        ave_alignment = ave_alignment + ave_alignment\n",
    "\n",
    "                # residual\n",
    "                if isinstance(f, Conv1dGLU):\n",
    "                    x = (x + residual) * math.sqrt(0.5)\n",
    "\n",
    "            decoder_state = x\n",
    "            x = self.last_conv.incremental_forward(x)\n",
    "            ave_alignment = ave_alignment.div_(num_attention_layers)\n",
    "\n",
    "            # Ooutput & done flag predictions\n",
    "            output = torch.sigmoid(x)\n",
    "            done = torch.sigmoid(self.fc(x))\n",
    "\n",
    "            decoder_states += [decoder_state]\n",
    "            outputs += [output]\n",
    "            alignments += [ave_alignment]\n",
    "            dones += [done]\n",
    "\n",
    "            t += 1\n",
    "            if test_inputs is None:\n",
    "                if (done > 0.5).all() and t > self.min_decoder_steps:\n",
    "                    break\n",
    "                elif t > self.max_decoder_steps:\n",
    "                    break\n",
    "\n",
    "        # Remove 1-element time axis\n",
    "        alignments = list(map(lambda x: x.squeeze(1), alignments))\n",
    "        decoder_states = list(map(lambda x: x.squeeze(1), decoder_states))\n",
    "        outputs = list(map(lambda x: x.squeeze(1), outputs))\n",
    "\n",
    "        # Combine outputs for all time steps\n",
    "        alignments = torch.stack(alignments).transpose(0, 1)\n",
    "        decoder_states = torch.stack(decoder_states).transpose(0, 1).contiguous()\n",
    "        outputs = torch.stack(outputs).transpose(0, 1).contiguous()\n",
    "\n",
    "        return outputs, alignments, dones, decoder_states\n",
    "\n",
    "    def start_fresh_sequence(self):\n",
    "        _clear_modules(self.preattention)\n",
    "        _clear_modules(self.convolutions)\n",
    "        self.last_conv.clear_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiSpeakerTTSModel(nn.Module):\n",
    "    \"\"\"Attention seq2seq model + post processing network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seq2seq, postnet,\n",
    "                 mel_dim=80, linear_dim=513,\n",
    "                 n_speakers=1, speaker_embed_dim=16, padding_idx=None,\n",
    "                 trainable_positional_encodings=False,\n",
    "                 use_decoder_state_for_postnet_input=False,\n",
    "                 speaker_embedding_weight_std=0.01,\n",
    "                 freeze_embedding=False):\n",
    "        super(MultiSpeakerTTSModel, self).__init__()\n",
    "        self.seq2seq = seq2seq\n",
    "        self.postnet = postnet  # referred as \"Converter\" in DeepVoice3\n",
    "        self.mel_dim = mel_dim\n",
    "        self.linear_dim = linear_dim\n",
    "        self.trainable_positional_encodings = trainable_positional_encodings\n",
    "        self.use_decoder_state_for_postnet_input = use_decoder_state_for_postnet_input\n",
    "        self.freeze_embedding = freeze_embedding\n",
    "\n",
    "        # Speaker embedding\n",
    "        if n_speakers > 1:\n",
    "            self.embed_speakers = Embedding(\n",
    "                n_speakers, speaker_embed_dim, padding_idx=None,\n",
    "                std=speaker_embedding_weight_std)\n",
    "        self.n_speakers = n_speakers\n",
    "        self.speaker_embed_dim = speaker_embed_dim\n",
    "\n",
    "    def make_generation_fast_(self):\n",
    "\n",
    "        def remove_weight_norm(m):\n",
    "            try:\n",
    "                nn.utils.remove_weight_norm(m)\n",
    "            except ValueError:  # this module didn't have weight norm\n",
    "                return\n",
    "        self.apply(remove_weight_norm)\n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        freezed_param_ids = set()\n",
    "\n",
    "        encoder, decoder = self.seq2seq.encoder, self.seq2seq.decoder\n",
    "\n",
    "        # Avoid updating the position encoding\n",
    "        if not self.trainable_positional_encodings:\n",
    "            pe_query_param_ids = set(map(id, decoder.embed_query_positions.parameters()))\n",
    "            pe_keys_param_ids = set(map(id, decoder.embed_keys_positions.parameters()))\n",
    "            freezed_param_ids |= (pe_query_param_ids | pe_keys_param_ids)\n",
    "        # Avoid updating the text embedding\n",
    "        if self.freeze_embedding:\n",
    "            embed_param_ids = set(map(id, encoder.embed_tokens.parameters()))\n",
    "            freezed_param_ids |= embed_param_ids\n",
    "\n",
    "        return (p for p in self.parameters() if id(p) not in freezed_param_ids)\n",
    "\n",
    "    def forward(self, text_sequences, mel_targets=None, speaker_ids=None,\n",
    "                text_positions=None, frame_positions=None, input_lengths=None):\n",
    "        B = text_sequences.size(0)\n",
    "\n",
    "        if speaker_ids is not None:\n",
    "            assert self.n_speakers > 1\n",
    "            speaker_embed = self.embed_speakers(speaker_ids)\n",
    "        else:\n",
    "            speaker_embed = None\n",
    "\n",
    "        # Apply seq2seq\n",
    "        # (B, T//r, mel_dim*r)\n",
    "        mel_outputs, alignments, done, decoder_states = self.seq2seq(\n",
    "            text_sequences, mel_targets, speaker_embed,\n",
    "            text_positions, frame_positions, input_lengths)\n",
    "\n",
    "        # Reshape\n",
    "        # (B, T, mel_dim)\n",
    "        mel_outputs = mel_outputs.view(B, -1, self.mel_dim)\n",
    "\n",
    "        # Prepare postnet inputs\n",
    "        if self.use_decoder_state_for_postnet_input:\n",
    "            postnet_inputs = decoder_states.view(B, mel_outputs.size(1), -1)\n",
    "        else:\n",
    "            postnet_inputs = mel_outputs\n",
    "\n",
    "        # (B, T, linear_dim)\n",
    "        # Convert coarse mel-spectrogram (or decoder hidden states) to\n",
    "        # high resolution spectrogram\n",
    "        linear_outputs = self.postnet(postnet_inputs, speaker_embed)\n",
    "        assert linear_outputs.size(-1) == self.linear_dim\n",
    "\n",
    "        return mel_outputs, linear_outputs, alignments, done\n",
    "\n",
    "\n",
    "class AttentionSeq2Seq(nn.Module):\n",
    "    \"\"\"Encoder + Decoder with attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(AttentionSeq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        if isinstance(self.decoder.attention, nn.ModuleList):\n",
    "            self.encoder.num_attention_layers = sum(\n",
    "                [layer is not None for layer in decoder.attention])\n",
    "\n",
    "    def forward(self, text_sequences, mel_targets=None, speaker_embed=None,\n",
    "                text_positions=None, frame_positions=None, input_lengths=None):\n",
    "        # (B, T, text_embed_dim)\n",
    "        encoder_outputs = self.encoder(\n",
    "            text_sequences, lengths=input_lengths, speaker_embed=speaker_embed)\n",
    "\n",
    "        # Mel: (B, T//r, mel_dim*r)\n",
    "        # Alignments: (N, B, T_target, T_input)\n",
    "        # Done: (B, T//r, 1)\n",
    "        mel_outputs, alignments, done, decoder_states = self.decoder(\n",
    "            encoder_outputs, mel_targets,\n",
    "            text_positions=text_positions, frame_positions=frame_positions,\n",
    "            speaker_embed=speaker_embed, lengths=input_lengths)\n",
    "\n",
    "        return mel_outputs, alignments, done, decoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL\n",
    "def deepvoice3(n_vocab, embed_dim=256, mel_dim=80, linear_dim=513, r=4,\n",
    "               downsample_step=1,\n",
    "               n_speakers=1, speaker_embed_dim=16, padding_idx=0,\n",
    "               dropout=(1 - 0.95), kernel_size=5,\n",
    "               encoder_channels=128,\n",
    "               decoder_channels=256,\n",
    "               converter_channels=256,\n",
    "               query_position_rate=1.0,\n",
    "               key_position_rate=1.29,\n",
    "               use_memory_mask=False,\n",
    "               trainable_positional_encodings=False,\n",
    "               force_monotonic_attention=True,\n",
    "               use_decoder_state_for_postnet_input=True,\n",
    "               max_positions=512,\n",
    "               embedding_weight_std=0.1,\n",
    "               speaker_embedding_weight_std=0.01,\n",
    "               freeze_embedding=False,\n",
    "               window_ahead=3,\n",
    "               window_backward=1,\n",
    "               key_projection=False,\n",
    "               value_projection=False,\n",
    "               ):\n",
    "    \"\"\"Build deepvoice3\n",
    "    \"\"\"\n",
    "    from deepvoice3_pytorch.deepvoice3 import Encoder, Decoder, Converter\n",
    "\n",
    "    time_upsampling = max(downsample_step // r, 1)\n",
    "\n",
    "    # Seq2seq\n",
    "    h = encoder_channels  # hidden dim (channels)\n",
    "    k = kernel_size   # kernel size\n",
    "    encoder = Encoder(\n",
    "        n_vocab, embed_dim, padding_idx=padding_idx,\n",
    "        n_speakers=n_speakers, speaker_embed_dim=speaker_embed_dim,\n",
    "        dropout=dropout, max_positions=max_positions,\n",
    "        embedding_weight_std=embedding_weight_std,\n",
    "        # (channels, kernel_size, dilation)\n",
    "        convolutions=[(h, k, 1), (h, k, 3), (h, k, 9), (h, k, 27),\n",
    "                      (h, k, 1), (h, k, 3), (h, k, 9), (h, k, 27),\n",
    "                      (h, k, 1), (h, k, 3)],\n",
    "    )\n",
    "\n",
    "    h = decoder_channels\n",
    "    decoder = Decoder(\n",
    "        embed_dim, in_dim=mel_dim, r=r, padding_idx=padding_idx,\n",
    "        n_speakers=n_speakers, speaker_embed_dim=speaker_embed_dim,\n",
    "        dropout=dropout, max_positions=max_positions,\n",
    "        preattention=[(h, k, 1), (h, k, 3)],\n",
    "        convolutions=[(h, k, 1), (h, k, 3), (h, k, 9), (h, k, 27),\n",
    "                      (h, k, 1)],\n",
    "        attention=[True, False, False, False, True],\n",
    "        force_monotonic_attention=force_monotonic_attention,\n",
    "        query_position_rate=query_position_rate,\n",
    "        key_position_rate=key_position_rate,\n",
    "        use_memory_mask=use_memory_mask,\n",
    "        window_ahead=window_ahead,\n",
    "        window_backward=window_backward,\n",
    "        key_projection=key_projection,\n",
    "        value_projection=value_projection,\n",
    "    )\n",
    "\n",
    "    seq2seq = AttentionSeq2Seq(encoder, decoder)\n",
    "\n",
    "    # Post net\n",
    "    if use_decoder_state_for_postnet_input:\n",
    "        in_dim = h // r\n",
    "    else:\n",
    "        in_dim = mel_dim\n",
    "    h = converter_channels\n",
    "    converter = Converter(\n",
    "        n_speakers=n_speakers, speaker_embed_dim=speaker_embed_dim,\n",
    "        in_dim=in_dim, out_dim=linear_dim, dropout=dropout,\n",
    "        time_upsampling=time_upsampling,\n",
    "        convolutions=[(h, k, 1), (h, k, 3), (2 * h, k, 1), (2 * h, k, 3)],\n",
    "    )\n",
    "\n",
    "    # Seq2seq + post net\n",
    "    model = MultiSpeakerTTSModel(\n",
    "        seq2seq, converter, padding_idx=padding_idx,\n",
    "        mel_dim=mel_dim, linear_dim=linear_dim,\n",
    "        n_speakers=n_speakers, speaker_embed_dim=speaker_embed_dim,\n",
    "        trainable_positional_encodings=trainable_positional_encodings,\n",
    "        use_decoder_state_for_postnet_input=use_decoder_state_for_postnet_input,\n",
    "        speaker_embedding_weight_std=speaker_embedding_weight_std,\n",
    "        freeze_embedding=freeze_embedding)\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
